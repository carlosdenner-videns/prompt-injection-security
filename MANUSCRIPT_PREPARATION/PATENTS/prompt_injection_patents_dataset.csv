publication_number,title,abstract,claims,assignees,inventor_names,cpc_codes,ipc_codes,publication_date,filing_date,priority_date,country_code,application_number,family_id,publication_year,full_text
CN-119249422-A,Detection method and detection system for universal privacy compliance,本发明提供了一种通用型隐私合规的检测方法以及检测系统，所述检测方法包括如下操作步骤：步骤一、敏感信息采集；步骤二、权限获取；步骤三、自动化检测，根据需要检测目标的类型，选择通用型检测方案或特定型检测方案，根据检测方案对隐私是否合规进行检测；步骤四、报告生成。本发明适用于中国的所有App应用程序，通过AOSP和jailbreak方式，实现了无侵入的隐私合规检测，同时使用了自动化技术，检测流程无需人工干预，自动生成检测结果的报告，使用人员仅需关注检测报告中标注的部分即可，同时仅检测平台跟进工信部的合规安全条例，应用程序所有方无需关注，实现了快速、高效的、通用的隐私合规检测。,,北京比特易湃信息技术有限公司,,,,20250103,20240919,20240919,CN,CN-202411310149-A,94024821,,
US-2025055867-A1,Dynamic threat mitigating of generative artificial intelligence models,"The disclosure relates to a method and system for dynamically mitigating threats of generative Artificial Intelligence (AI) models. Conventional systems often suffer from inefficiencies due to sequentially applying threat detection checks leading to unnecessary preprocessing and increased computational demands. Additionally, such systems typically focus only on input data, neglecting potential threats in outputs. The disclosed system and method addresses these drawbacks by employing a hierarchical structure of macro and nano classifiers. The system utilizes macro classifiers for broad initial threat categorization followed by specialized nano classifiers for detailed analysis of specific threat subtypes, thereby optimizing processing time and computational resources. The system operates in real time, applying predefined moderation rules to both input and output data to ensure comprehensive threat mitigation. Additionally, continuous telemetry data updates refine nano classifiers and threat identification mechanisms, maintaining high accuracy and adaptability. The disclosed method enhances safety efficiency and reliability of generative AI models.","What is claimed is: 
     
         1 . A processor implemented method for dynamically mitigating threats of a generative Artificial Intelligence (AI) model, the method comprising:
 receiving, via one or more hardware processors, data associated with the generative AI model at a user interface (UI) of a computing device, the data associated with one or more attributes;   applying, via the one or more hardware processors, one or more macro classifiers to the data to determine, in real time, presence of one or more types of threats from amongst a plurality of types of threats, wherein each macro classifier of the one or more macro classifiers is capable of computing a first threat probability score associated with a type of threat from amongst the plurality of types of threats based on the one or more attributes associated with the data;   dynamically configuring, via the one or more hardware processors, a threat detection model comprising one or more nano classifiers to detect one or more sub-type of threats associated with the one or more type of threats in the data, wherein dynamically configuring the threat detection model comprises:   selectively moderating the data, via the one or more hardware processors, based on one or more predefined rules corresponding to each of the one or more sub-type of threats to obtain a moderated data; and   validating, via the one or more hardware processors, the moderated data to determine one of presence and absence of the one or more sub-types of threats in the moderated data.   
     
     
         2 . The processor implemented method of  claim 1 , wherein the data comprises at least one of an input data and an output data. 
     
     
         3 . The processor implemented method of  claim 1 , wherein the data comprises multi-modal data, the multi-modal data comprising one or more of textual data, audio data, video data, and image data. 
     
     
         4 . The processor implemented method of  claim 2 , wherein each attribute from amongst the one or more attributes comprises one of nature of the input data, nature of the output data, usage history and context associated with the data, and similarity with past violations and threats. 
     
     
         5 . The processor implemented method of  claim 1 , wherein dynamically configuring the threat detection model comprises:
 selecting, from a database, one or more nano classifiers from amongst a plurality of nano classifiers selectively trained to detect the one or more sub-type of threats in the data;   computing, for each of the one or more sub-types of threats, a second threat probability score by the one or more nano classifiers; and   comparing, for each of the one or more sub-types of the threats, the second threat probability score with a predefined threshold value of the second threat probability score to detect the presence of the one or more sub-types of the threats in the data.   
     
     
         6 . The processor implemented method of  claim 1 , wherein a type of threat from amongst the plurality of types of threats is one of a prompt injection threat, a jailbreak threat, a profanity threat, a toxicity threat, a Personal Identifiable Information (PII) leakage threat, an Intellectual Property (IP) violation threat, an organization policy and role-based threat, a hallucination threat, security attacks, and sensitive information leakage threat. 
     
     
         7 . The processor implemented method of  claim 1 , wherein the one or more nano classifiers comprises at least one of one or more Machine Learning (ML) models, one or more deep learning models, one or more transfer learning models, one or more rule-based repositories, one or more datasets and dictionaries, one or more custom and finetuned models, one or more knowledge databases, one or more Retrieval Augmented Generation (RAG) model, and a reminder generation model. 
     
     
         8 . The processor implemented method of  claim 1 , wherein moderating the data comprises performing at least one of filtering and rephrasing at least a portion of the data. 
     
     
         9 . The processor implemented method of  claim 1 , wherein validating the moderated data comprises iteratively:
 computing, for each of the one or more sub-types of threats in the moderated data, the second threat probability score;   comparing the second threat probability score with the predefined threshold value of the second threat probability score; and   moderating the moderated data, until the second threat probability score is determined to be less than the predefined threshold value of the second threat probability score, wherein the second threat probability score being less than the predefined threshold value is indicative of absence of the one or more sub-types of threats in the moderated data.   
     
     
         10 . The processor implemented method of  claim 9 , further comprises:
 restricting moderating the moderated data upon determining the second threat probability score greater than or equal to the predefined threshold value of the second threat probability score for a predefined number of iterations; and   rendering details of restricting the moderated data on the UI.   
     
     
         11 . The processor implemented method of  claim 1 , further comprising learning a set of emerging types of threats and a set of emerging sub-types of threats using reinforcement learning with human feedback (RHFL). 
     
     
         12 . The processor implemented method of  claim 1 , further comprising:
 tracking a telemetry status of the data; and   updating the one or more nano classifiers, the threat detection model, and one or more policies associated with an entity implementing the generative AI model based on the telemetry status of the data.   
     
     
         13 . A system for dynamically mitigating threats of a generative Artificial Intelligence (AI) model, the system comprising:
 one or more hardware processors; and   a memory communicatively coupled to the one or more hardware processors, wherein the memory stores processor-executable instructions, which, on execution, causes the one or more hardware processors to:
 receive data associated with a generative AI model at a user interface (UI) of a computing device, the data associated with one or more attributes; 
 apply one or more macro classifiers to the data to determine, in real time, presence of one or more types of threats from amongst a plurality of types of threats, wherein each macro classifier of the one or more macro classifiers is capable of computing a first threat probability score associated with a type of threat from amongst the plurality of types of threats based on the one or more attributes associated with the data; 
 dynamically configure a threat detection model comprising one or more nano classifiers to detect one or more sub-type of threats associated with the one or more type of threats in the data; 
   selectively moderate the data based on one or more predefined rules corresponding to each of the one or more sub-type of threats to obtain a moderated data; and   validate the moderated data to determine one of presence and absence of the one or more sub-types of threats in the moderated data.   
     
     
         14 . The system of  claim 13 , wherein the data is at least one of an input data or output data, and wherein the data is multi-modal data comprising one or more of a text, an audio, a video, and an image. 
     
     
         15 . The system of  claim 13 , wherein to dynamically configure the threat detection model, the one or more hardware processors are configured by the instructions to:
 select, from a database, one or more nano classifiers from amongst a plurality of nano classifiers selectively trained to detect the one or more sub-type of threats in the data;   compute, for each of the one or more sub-types of threats, a second threat probability score by the one or more nano classifiers; and   compare, for each of the one or more sub-types of the threats, the second threat probability score with a predefined threshold value of the second threat probability score to detect the presence of the one or more sub-types of the threats in the data.   
     
     
         16 . The system of  claim 13 , wherein to moderate the data, the processor-executable instructions further cause the one or more hardware processors to perform at least one of filtering and rephrasing at least a portion of the data. 
     
     
         17 . The system of  claim 13 , wherein to validate the moderated data, the processor-executable instructions further cause the one or more hardware processors to iteratively:
 compute, for each of the one or more sub-types of threats in the moderated data, the second threat probability score;   compare the second threat probability score with the predefined threshold value of the second threat probability score; and   moderate the moderated data, until the second threat probability score is determined to be less than the predefined threshold value of the second threat probability score, wherein the second threat probability score being less than the predefined threshold value is indicative of absence of the one or more sub-types of threats in the moderated data.   
     
     
         18 . The system of  claim 17 , wherein the processor-executable instructions further cause the one or more hardware processors to:
 restrict moderating the moderated data upon determining the second threat probability score greater than or equal to the predefined threshold value of the second threat probability score for a predefined number of iterations; and   render details of restricting the moderated data on the UI.   
     
     
         19 . The system of  claim 13 , wherein the processor-executable instructions further cause the one or more hardware processors to:
 track a telemetry status of the data; and   
       update the one or more nano classifiers, the threat detection model, and one or more policies associated with an entity implementing the generative AI model based on the telemetry status of the data. 
     
     
         20 . A non-transitory computer-readable medium storing computer-executable instructions for dynamically mitigating threats of a generative Artificial Intelligence (AI) model, the stored computer-executable instructions, when executed by one or more hardware processors, cause the one or more hardware processors to perform operations comprising:
 receiving data associated with a generative AI model at a user interface (UI) of a computing device, the data associated with one or more attributes;   applying one or more macro classifiers to the data to determine, in real time, presence of one or more types of threats from amongst a plurality of types of threats, wherein each macro classifier of the one or more macro classifiers is capable of computing a first threat probability score associated with a type of threat from amongst the plurality of types of threats based on the one or more attributes associated with the data;   dynamically configuring a threat detection model comprising one or more nano classifiers to detect one or more sub-type of threats associated with the one or more types of threats in the data;   selectively moderating the data based on one or more predefined rules corresponding to each of the one or more sub-type of threats to obtain a moderated data; and   validating the moderated data to determine one of presence and absence of the one or more sub-types of threats in the moderated data.",Infosys Limited,,H04L63/205 | H04L63/1425,H04L9/40,2025-02-13,20240808,20230810,US,US-202418797798-A,-1,2025.0,"dynamic threat mitigating of generative artificial intelligence models the disclosure relates to a method and system for dynamically mitigating threats of generative artificial intelligence (ai) models. conventional systems often suffer from inefficiencies due to sequentially applying threat detection checks leading to unnecessary preprocessing and increased computational demands. additionally, such systems typically focus only on input data, neglecting potential threats in outputs. the disclosed system and method addresses these drawbacks by employing a hierarchical structure of macro and nano classifiers. the system utilizes macro classifiers for broad initial threat categorization followed by specialized nano classifiers for detailed analysis of specific threat subtypes, thereby optimizing processing time and computational resources. the system operates in real time, applying predefined moderation rules to both input and output data to ensure comprehensive threat mitigation. additionally, continuous telemetry data updates refine nano classifiers and threat identification mechanisms, maintaining high accuracy and adaptability. the disclosed method enhances safety efficiency and reliability of generative ai models. what is claimed is: 
     
         1 . a processor implemented method for dynamically mitigating threats of a generative artificial intelligence (ai) model, the method comprising:
 receiving, via one or more hardware processors, data associated with the generative ai model at a user interface (ui) of a computing device, the data associated with one or more attributes;   applying, via the one or more hardware processors, one or more macro classifiers to the data to determine, in real time, presence of one or more types of threats from amongst a plurality of types of threats, wherein each macro classifier of the one or more macro classifiers is capable of computing a first threat probability score associated with a type of threat from amongst the plurality of types of threats based on the one or more attributes associated with the data;   dynamically configuring, via the one or more hardware processors, a threat detection model comprising one or more nano classifiers to detect one or more sub-type of threats associated with the one or more type of threats in the data, wherein dynamically configuring the threat detection model comprises:   selectively moderating the data, via the one or more hardware processors, based on one or more predefined rules corresponding to each of the one or more sub-type of threats to obtain a moderated data; and   validating, via the one or more hardware processors, the moderated data to determine one of presence and absence of the one or more sub-types of threats in the moderated data.   
     
     
         2 . the processor implemented method of  claim 1 , wherein the data comprises at least one of an input data and an output data. 
     
     
         3 . the processor implemented method of  claim 1 , wherein the data comprises multi-modal data, the multi-modal data comprising one or more of textual data, audio data, video data, and image data. 
     
     
         4 . the processor implemented method of  claim 2 , wherein each attribute from amongst the one or more attributes comprises one of nature of the input data, nature of the output data, usage history and context associated with the data, and similarity with past violations and threats. 
     
     
         5 . the processor implemented method of  claim 1 , wherein dynamically configuring the threat detection model comprises:
 selecting, from a database, one or more nano classifiers from amongst a plurality of nano classifiers selectively trained to detect the one or more sub-type of threats in the data;   computing, for each of the one or more sub-types of threats, a second threat probability score by the one or more nano classifiers; and   comparing, for each of the one or more sub-types of the threats, the second threat probability score with a predefined threshold value of the second threat probability score to detect the presence of the one or more sub-types of the threats in the data.   
     
     
         6 . the processor implemented method of  claim 1 , wherein a type of threat from amongst the plurality of types of threats is one of a prompt injection threat, a jailbreak threat, a profanity threat, a toxicity threat, a personal identifiable information (pii) leakage threat, an intellectual property (ip) violation threat, an organization policy and role-based threat, a hallucination threat, security attacks, and sensitive information leakage threat. 
     
     
         7 . the processor implemented method of  claim 1 , wherein the one or more nano classifiers comprises at least one of one or more machine learning (ml) models, one or more deep learning models, one or more transfer learning models, one or more rule-based repositories, one or more datasets and dictionaries, one or more custom and finetuned models, one or more knowledge databases, one or more retrieval augmented generation (rag) model, and a reminder generation model. 
     
     
         8 . the processor implemented method of  claim 1 , wherein moderating the data comprises performing at least one of filtering and rephrasing at least a portion of the data. 
     
     
         9 . the processor implemented method of  claim 1 , wherein validating the moderated data comprises iteratively:
 computing, for each of the one or more sub-types of threats in the moderated data, the second threat probability score;   comparing the second threat probability score with the predefined threshold value of the second threat probability score; and   moderating the moderated data, until the second threat probability score is determined to be less than the predefined threshold value of the second threat probability score, wherein the second threat probability score being less than the predefined threshold value is indicative of absence of the one or more sub-types of threats in the moderated data.   
     
     
         10 . the processor implemented method of  claim 9 , further comprises:
 restricting moderating the moderated data upon determining the second threat probability score greater than or equal to the predefined threshold value of the second threat probability score for a predefined number of iterations; and   rendering details of restricting the moderated data on the ui.   
     
     
         11 . the processor implemented method of  claim 1 , further comprising learning a set of emerging types of threats and a set of emerging sub-types of threats using reinforcement learning with human feedback (rhfl). 
     
     
         12 . the processor implemented method of  claim 1 , further comprising:
 tracking a telemetry status of the data; and   updating the one or more nano classifiers, the threat detection model, and one or more policies associated with an entity implementing the generative ai model based on the telemetry status of the data.   
     
     
         13 . a system for dynamically mitigating threats of a generative artificial intelligence (ai) model, the system comprising:
 one or more hardware processors; and   a memory communicatively coupled to the one or more hardware processors, wherein the memory stores processor-executable instructions, which, on execution, causes the one or more hardware processors to:
 receive data associated with a generative ai model at a user interface (ui) of a computing device, the data associated with one or more attributes; 
 apply one or more macro classifiers to the data to determine, in real time, presence of one or more types of threats from amongst a plurality of types of threats, wherein each macro classifier of the one or more macro classifiers is capable of computing a first threat probability score associated with a type of threat from amongst the plurality of types of threats based on the one or more attributes associated with the data; 
 dynamically configure a threat detection model comprising one or more nano classifiers to detect one or more sub-type of threats associated with the one or more type of threats in the data; 
   selectively moderate the data based on one or more predefined rules corresponding to each of the one or more sub-type of threats to obtain a moderated data; and   validate the moderated data to determine one of presence and absence of the one or more sub-types of threats in the moderated data.   
     
     
         14 . the system of  claim 13 , wherein the data is at least one of an input data or output data, and wherein the data is multi-modal data comprising one or more of a text, an audio, a video, and an image. 
     
     
         15 . the system of  claim 13 , wherein to dynamically configure the threat detection model, the one or more hardware processors are configured by the instructions to:
 select, from a database, one or more nano classifiers from amongst a plurality of nano classifiers selectively trained to detect the one or more sub-type of threats in the data;   compute, for each of the one or more sub-types of threats, a second threat probability score by the one or more nano classifiers; and   compare, for each of the one or more sub-types of the threats, the second threat probability score with a predefined threshold value of the second threat probability score to detect the presence of the one or more sub-types of the threats in the data.   
     
     
         16 . the system of  claim 13 , wherein to moderate the data, the processor-executable instructions further cause the one or more hardware processors to perform at least one of filtering and rephrasing at least a portion of the data. 
     
     
         17 . the system of  claim 13 , wherein to validate the moderated data, the processor-executable instructions further cause the one or more hardware processors to iteratively:
 compute, for each of the one or more sub-types of threats in the moderated data, the second threat probability score;   compare the second threat probability score with the predefined threshold value of the second threat probability score; and   moderate the moderated data, until the second threat probability score is determined to be less than the predefined threshold value of the second threat probability score, wherein the second threat probability score being less than the predefined threshold value is indicative of absence of the one or more sub-types of threats in the moderated data.   
     
     
         18 . the system of  claim 17 , wherein the processor-executable instructions further cause the one or more hardware processors to:
 restrict moderating the moderated data upon determining the second threat probability score greater than or equal to the predefined threshold value of the second threat probability score for a predefined number of iterations; and   render details of restricting the moderated data on the ui.   
     
     
         19 . the system of  claim 13 , wherein the processor-executable instructions further cause the one or more hardware processors to:
 track a telemetry status of the data; and   
       update the one or more nano classifiers, the threat detection model, and one or more policies associated with an entity implementing the generative ai model based on the telemetry status of the data. 
     
     
         20 . a non-transitory computer-readable medium storing computer-executable instructions for dynamically mitigating threats of a generative artificial intelligence (ai) model, the stored computer-executable instructions, when executed by one or more hardware processors, cause the one or more hardware processors to perform operations comprising:
 receiving data associated with a generative ai model at a user interface (ui) of a computing device, the data associated with one or more attributes;   applying one or more macro classifiers to the data to determine, in real time, presence of one or more types of threats from amongst a plurality of types of threats, wherein each macro classifier of the one or more macro classifiers is capable of computing a first threat probability score associated with a type of threat from amongst the plurality of types of threats based on the one or more attributes associated with the data;   dynamically configuring a threat detection model comprising one or more nano classifiers to detect one or more sub-type of threats associated with the one or more types of threats in the data;   selectively moderating the data based on one or more predefined rules corresponding to each of the one or more sub-type of threats to obtain a moderated data; and   validating the moderated data to determine one of presence and absence of the one or more sub-types of threats in the moderated data."
CN-118551366-A,"Prompt injection attack defense method and device, storage medium and electronic equipment",本说明书实施例公开了一种提示注入攻击防御方法、装置、存储介质及电子设备，首先，在检测到向部署的大语言模型发起的当前提问请求时，通过预注入的安全切面服务拦截所述当前提问请求，并通过安全切面服务对当前提问请求进行伪装指令消除，得到目标提问请求，将目标提问请求输入到大语言模型中，以确定当前提问请求对应的安全回答反馈数据。本技术方案能够有效避免提示注入攻击，且不依赖于专家先验知识，提升提示注入攻击的防御准确性，提升大语言模型服务的安全性；同时，基于安全切面服务也不需要调整大语言模型的基础架构，有效降低提示注入攻击防御的工作量，降低成本，且安全切面服务的防御响应迅速，进一步提升安全性。,,蚂蚁科技集团股份有限公司,LIU YAN | WENG HAIQIN,G06N3/0455 | G06N3/088 | G06N5/041 | G06F16/3329 | G06F21/554 | G06F21/54,G06N5/04 | G06N3/088 | G06N3/0455 | G06F21/55 | G06F16/332 | G06F21/54,2024-08-27,20240730,20240730,CN,CN-202411031026-A,92444736,2024.0,"prompt injection attack defense method and device, storage medium and electronic equipment 本说明书实施例公开了一种提示注入攻击防御方法、装置、存储介质及电子设备，首先，在检测到向部署的大语言模型发起的当前提问请求时，通过预注入的安全切面服务拦截所述当前提问请求，并通过安全切面服务对当前提问请求进行伪装指令消除，得到目标提问请求，将目标提问请求输入到大语言模型中，以确定当前提问请求对应的安全回答反馈数据。本技术方案能够有效避免提示注入攻击，且不依赖于专家先验知识，提升提示注入攻击的防御准确性，提升大语言模型服务的安全性；同时，基于安全切面服务也不需要调整大语言模型的基础架构，有效降低提示注入攻击防御的工作量，降低成本，且安全切面服务的防御响应迅速，进一步提升安全性。 "
CN-118965338-A,一种大模型提示词注入防御方法及装置,"The invention provides a large model prompt word injection defense method and a device, wherein the method comprises the following steps: on a prompt word construction side, acquiring input content of a user, identifying an input risk type according to the input content, generating corresponding safety early warning content according to the input risk type, adding the safety early warning content to the input content, constructing an input prompt, and sending the input prompt to a large language model; and generating output content according to the input prompt input by the large language model. The invention sets a section of safety early warning content behind the input content of the user so as to enhance the safety and effectiveness of the input prompt, ensure that the input prompt is strengthened and safely processed before entering the large language model, reduce the influence of the unsafe prompt on the output of the model, and further improve the safety of the large language model.",,福建福诺移动通信技术有限公司,CHEN FAN | LIN SHIXIONG | YAO RUIXIANG,,G06N20/00 | G06F21/64 | G06F21/57 | G06F21/55,2024-11-15,20240708,20240708,CN,CN-202410906968-A,93390157,2024.0,"一种大模型提示词注入防御方法及装置 the invention provides a large model prompt word injection defense method and a device, wherein the method comprises the following steps: on a prompt word construction side, acquiring input content of a user, identifying an input risk type according to the input content, generating corresponding safety early warning content according to the input risk type, adding the safety early warning content to the input content, constructing an input prompt, and sending the input prompt to a large language model; and generating output content according to the input prompt input by the large language model. the invention sets a section of safety early warning content behind the input content of the user so as to enhance the safety and effectiveness of the input prompt, ensure that the input prompt is strengthened and safely processed before entering the large language model, reduce the influence of the unsafe prompt on the output of the model, and further improve the safety of the large language model. "
CN-118656822-A,一种语言模型的风险检测方法、装置、设备、介质、产品,"The application discloses a risk detection method, a device, equipment, a medium and a product of a language model, wherein the method comprises the following steps: after model input data of a target language model is acquired, determining at least one of intention description content and manipulation description content from the model input data; then, carrying out intention risk detection processing on the intention descriptive content to obtain an intention risk detection result, and/or carrying out manipulation risk detection processing on the manipulation descriptive content to obtain a manipulation risk detection result; and finally, determining a risk detection result of the target language model under the model input data according to the intention risk detection result and/or the manipulation risk detection result, so that the risk detection result can indicate whether risks exist, such as a Prompt injection risk, and the like, thereby effectively avoiding the security problem caused by the risks and further being beneficial to improving the security of the business application based on the language model.",,北京火山引擎科技有限公司,DUAN XU | LIU YANNAN,G06F21/71 | G06F21/554,G06F21/71 | G06F21/55,2024-09-17,20240626,20240626,CN,CN-202410841701-A,92703783,2024.0,"一种语言模型的风险检测方法、装置、设备、介质、产品 the application discloses a risk detection method, a device, equipment, a medium and a product of a language model, wherein the method comprises the following steps: after model input data of a target language model is acquired, determining at least one of intention description content and manipulation description content from the model input data; then, carrying out intention risk detection processing on the intention descriptive content to obtain an intention risk detection result, and/or carrying out manipulation risk detection processing on the manipulation descriptive content to obtain a manipulation risk detection result; and finally, determining a risk detection result of the target language model under the model input data according to the intention risk detection result and/or the manipulation risk detection result, so that the risk detection result can indicate whether risks exist, such as a prompt injection risk, and the like, thereby effectively avoiding the security problem caused by the risks and further being beneficial to improving the security of the business application based on the language model. "
CN-118503361-A,Dynamic prompt injection method and system for large natural language processing model,"The application relates to the technical field of natural language processing, in particular to a dynamic prompt injection method and a system for a large natural language processing model. The method comprises the following steps: semantic understanding is carried out on the problem raised by the user by utilizing a pre-trained language model; generating supplementary Query and filling prompt according to semantic dynamics of the problem, and giving semantic understanding results to generate a prompt and a Query related to user intention; searching vector library data by using the generated query input to obtain a search result; fusing the search result and the template of the user intention into a new template, and injecting the new template into the large model to generate a final answer. The application improves the accuracy and the effectiveness of the large model when the user inquiry is responded, and can effectively improve the application effect of the NLP large model in the retrieval direction.",,山东浪潮科学研究院有限公司,ZHAN ENHAO | LI XUE | DUAN QIANG,G06F18/25 | G06N3/0455 | G06F40/30 | G06F16/38 | G06F16/3344,G06N3/0455 | G06F40/30 | G06F18/25 | G06F16/38 | G06F16/33,2024-08-16,20240531,20240531,CN,CN-202410700704-A,92234245,2024.0,"dynamic prompt injection method and system for large natural language processing model the application relates to the technical field of natural language processing, in particular to a dynamic prompt injection method and a system for a large natural language processing model. the method comprises the following steps: semantic understanding is carried out on the problem raised by the user by utilizing a pre-trained language model; generating supplementary query and filling prompt according to semantic dynamics of the problem, and giving semantic understanding results to generate a prompt and a query related to user intention; searching vector library data by using the generated query input to obtain a search result; fusing the search result and the template of the user intention into a new template, and injecting the new template into the large model to generate a final answer. the application improves the accuracy and the effectiveness of the large model when the user inquiry is responded, and can effectively improve the application effect of the nlp large model in the retrieval direction. "
US-12130917-B1,GenAI prompt injection classifier training using prompt attack structures,"An analysis engine receives data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model. The analysis engine, using a prompt injection classifier determines whether the prompt comprises or is indicative of malicious content or otherwise elicits malicious actions. The prompt injection classifier can be trained using a dataset generated by populating benign content and malicious content into a plurality of different prompt attack structures at pre-defined locations. Data characterizing the determination is provided to a consuming application or process. Related apparatus, systems, techniques and articles are also described.","What is claimed is: 
     
       1. A computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model; 
 determining, using a prompt injection classifier, whether the prompt comprises malicious content or elicits malicious actions; and 
 providing data characterizing the determination to a consuming application or process; 
 wherein the prompt injection classifier is trained using a dataset generated by populating benign content and malicious content into a plurality of different prompt attack structures, the attack structures each being a template in which at least one first field is designated for benign content and at least one second field is designated for malicious content. 
 
     
     
       2. The method of  claim 1 , wherein at least a portion of the malicious content is generated by instructing a misaligned large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the misaligned large language model having been fine-tuned to output malicious strings. 
     
     
       3. The method of  claim 1 , wherein at least a portion of the malicious content is generated by instructing a jailbroken large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the jailbroken large language model having been prompted with a specific input that allows it to respond with malicious strings. 
     
     
       4. The method of  claim 1 , wherein at least a portion of the malicious content is generated by instructing a large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the large language model having not been aligned in a way that restricts its output. 
     
     
       5. The method of  claim 1 , wherein at least a portion of the benign content is generated using a large language model with guardrails to generate a plurality of strings known to be benign. 
     
     
       6. The method of  claim 1 , wherein at least a portion of the benign content comprises human-generated content. 
     
     
       7. The method of  claim 1 , wherein at least a portion of the malicious content comprises human-generated content. 
     
     
       8. The method of  claim 1 , wherein the GenAI model comprises a large language model. 
     
     
       9. The method of  claim 1 , wherein the consuming application or process prevents the prompt from being input into the GenAI model upon a determination that the prompt comprises or elicits malicious content. 
     
     
       10. The method of  claim 1 , wherein the consuming application or process allows the prompt to be input into the GenAI model upon a determination that the prompt does not comprise or elicit malicious content. 
     
     
       11. The method of  claim 1 , wherein the consuming application or process flags the prompt as being malicious for quality assurance upon a determination that the prompt comprises or elicits malicious content. 
     
     
       12. The method of  claim 1 , wherein the consuming application or process modifies the prompt to be benign upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the GenAI model. 
     
     
       13. The method of  claim 1 , wherein the consuming application or process blocks an internet protocol (IP) address of a requester of the prompt upon a determination that the prompt comprises or elicits malicious content. 
     
     
       14. The method of  claim 1 , wherein the consuming application or process causes subsequent prompts from an entity identified by one or more of an internet protocol (IP) address, a media access control (MAC) address, or a session identifier of a requester of the prompt to be modified upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the GenAI model. 
     
     
       15. The method of  claim 1 , wherein the prompt attack structures are based on one or more of: a direct task deflection attack, a special case attack, a context continuation attack, a context termination attack, a syntactic transformation attack, an encryption attack, or a text redirection attack. 
     
     
       16. A computer-implemented method comprising:
 training a prompt injection classifier using a dataset generated by populating benign content and malicious content into a plurality of different prompt attack structures at pre-defined locations which are respectively tagged as being either for benign content or for malicious content; and 
 causing the trained prompt injection classifier to be deployed to determine whether prompts to be ingested by a generative artificial intelligence (GenAI) model comprise malicious content or elicits malicious actions. 
 
     
     
       17. The method of  claim 16 , wherein at least a portion of the malicious content is generated by instructing a misaligned large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the misaligned large language model having been fine-tuned to output malicious strings. 
     
     
       18. The method of  claim 16 , wherein at least a portion of the malicious content is generated by instructing a jailbroken large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the jailbroken large language model having been prompted with a specific input that allows it to respond with malicious strings. 
     
     
       19. The method of  claim 16 , wherein at least a portion of the malicious content is generated by instructing a large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the large language model having not been aligned in a way that restricts its output. 
     
     
       20. A computer-implemented method comprising:
 generating malicious content by instructing a misaligned or jailbroken first large language model to generate malicious strings having malicious content or eliciting malicious actions; 
 generating benign content by instructing a second large language model to generate benign strings having benign content; and 
 generating at least a portion of a training dataset by populating a plurality of different prompt attack structures with generated benign strings at locations tagged in the structures as being benign and with generated malicious strings at locations tagged in the structures as being malicious. 
 
     
     
       21. The method of  claim 20  further comprising:
 training a prompt injection classifier using the training dataset; and 
 deploying the trained prompt injection classifier to determine whether prompts to be ingested by a generative artificial intelligence (GenAI) model comprise malicious content or elicits malicious actions.","HiddenLayer, Inc.","YEUNG, KENNETH | Burns, Tanner | CAPPEL, KWESI",G06F2221/034 | G06F21/56 | G06F2221/034 | G06F21/56 | G06F2221/034 | G06N3/08 | G06N3/0475 | G06F21/56,G06N3/08 | G06N3/0475 | G06F21/56,2024-10-29,20240528,20240528,US,US-202418676190-A,93217148,2024.0,"genai prompt injection classifier training using prompt attack structures an analysis engine receives data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model. the analysis engine, using a prompt injection classifier determines whether the prompt comprises or is indicative of malicious content or otherwise elicits malicious actions. the prompt injection classifier can be trained using a dataset generated by populating benign content and malicious content into a plurality of different prompt attack structures at pre-defined locations. data characterizing the determination is provided to a consuming application or process. related apparatus, systems, techniques and articles are also described. what is claimed is: 
     
       1. a computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model; 
 determining, using a prompt injection classifier, whether the prompt comprises malicious content or elicits malicious actions; and 
 providing data characterizing the determination to a consuming application or process; 
 wherein the prompt injection classifier is trained using a dataset generated by populating benign content and malicious content into a plurality of different prompt attack structures, the attack structures each being a template in which at least one first field is designated for benign content and at least one second field is designated for malicious content. 
 
     
     
       2. the method of  claim 1 , wherein at least a portion of the malicious content is generated by instructing a misaligned large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the misaligned large language model having been fine-tuned to output malicious strings. 
     
     
       3. the method of  claim 1 , wherein at least a portion of the malicious content is generated by instructing a jailbroken large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the jailbroken large language model having been prompted with a specific input that allows it to respond with malicious strings. 
     
     
       4. the method of  claim 1 , wherein at least a portion of the malicious content is generated by instructing a large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the large language model having not been aligned in a way that restricts its output. 
     
     
       5. the method of  claim 1 , wherein at least a portion of the benign content is generated using a large language model with guardrails to generate a plurality of strings known to be benign. 
     
     
       6. the method of  claim 1 , wherein at least a portion of the benign content comprises human-generated content. 
     
     
       7. the method of  claim 1 , wherein at least a portion of the malicious content comprises human-generated content. 
     
     
       8. the method of  claim 1 , wherein the genai model comprises a large language model. 
     
     
       9. the method of  claim 1 , wherein the consuming application or process prevents the prompt from being input into the genai model upon a determination that the prompt comprises or elicits malicious content. 
     
     
       10. the method of  claim 1 , wherein the consuming application or process allows the prompt to be input into the genai model upon a determination that the prompt does not comprise or elicit malicious content. 
     
     
       11. the method of  claim 1 , wherein the consuming application or process flags the prompt as being malicious for quality assurance upon a determination that the prompt comprises or elicits malicious content. 
     
     
       12. the method of  claim 1 , wherein the consuming application or process modifies the prompt to be benign upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the genai model. 
     
     
       13. the method of  claim 1 , wherein the consuming application or process blocks an internet protocol (ip) address of a requester of the prompt upon a determination that the prompt comprises or elicits malicious content. 
     
     
       14. the method of  claim 1 , wherein the consuming application or process causes subsequent prompts from an entity identified by one or more of an internet protocol (ip) address, a media access control (mac) address, or a session identifier of a requester of the prompt to be modified upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the genai model. 
     
     
       15. the method of  claim 1 , wherein the prompt attack structures are based on one or more of: a direct task deflection attack, a special case attack, a context continuation attack, a context termination attack, a syntactic transformation attack, an encryption attack, or a text redirection attack. 
     
     
       16. a computer-implemented method comprising:
 training a prompt injection classifier using a dataset generated by populating benign content and malicious content into a plurality of different prompt attack structures at pre-defined locations which are respectively tagged as being either for benign content or for malicious content; and 
 causing the trained prompt injection classifier to be deployed to determine whether prompts to be ingested by a generative artificial intelligence (genai) model comprise malicious content or elicits malicious actions. 
 
     
     
       17. the method of  claim 16 , wherein at least a portion of the malicious content is generated by instructing a misaligned large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the misaligned large language model having been fine-tuned to output malicious strings. 
     
     
       18. the method of  claim 16 , wherein at least a portion of the malicious content is generated by instructing a jailbroken large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the jailbroken large language model having been prompted with a specific input that allows it to respond with malicious strings. 
     
     
       19. the method of  claim 16 , wherein at least a portion of the malicious content is generated by instructing a large language model to generate a plurality of strings having malicious content or eliciting malicious actions, the large language model having not been aligned in a way that restricts its output. 
     
     
       20. a computer-implemented method comprising:
 generating malicious content by instructing a misaligned or jailbroken first large language model to generate malicious strings having malicious content or eliciting malicious actions; 
 generating benign content by instructing a second large language model to generate benign strings having benign content; and 
 generating at least a portion of a training dataset by populating a plurality of different prompt attack structures with generated benign strings at locations tagged in the structures as being benign and with generated malicious strings at locations tagged in the structures as being malicious. 
 
     
     
       21. the method of  claim 20  further comprising:
 training a prompt injection classifier using the training dataset; and 
 deploying the trained prompt injection classifier to determine whether prompts to be ingested by a generative artificial intelligence (genai) model comprise malicious content or elicits malicious actions."
WO-2024238244-A1,Signing large language model prompts to prevent prompt injection attacks,"Une technique pour empêcher une attaque par injection de requête utilise un agent de sécurité pour signer une requête de grand modèle de langage avec un secret qui est isolé de l&#39;application ou du dispositif d&#39;utilisateur qui génère une requête d&#39;utilisateur. Le secret est adapté à un identifiant d&#39;utilisateur et à un identifiant de session spécifiques. Le grand modèle de langage reçoit l&#39;instruction de répéter le secret dans chaque réponse. L&#39;agent de sécurité extrait la réponse du grand modèle de langage et vérifie le secret. Lorsque le secret ne fait pas partie de la réponse, un message d&#39;erreur est transmis à l&#39;application d&#39;utilisateur au lieu de la réponse.",,"Microsoft Technology Licensing, Llc","CLEMENT, Colin | FU, SHENGYU | SUNDARESAN, NEELAKANTAN | YOU, Dongjiang",G06F21/554 | G06F40/35 | G06F40/30 | G06N3/08 | H04L63/1466,H04L9/40 | G06N3/08 | G06F40/35 | G06F40/30 | G06F21/55,2024-11-21,20240509,20230517,WO,US-2024028452-W,91376706,2024.0,"signing large language model prompts to prevent prompt injection attacks une technique pour empêcher une attaque par injection de requête utilise un agent de sécurité pour signer une requête de grand modèle de langage avec un secret qui est isolé de l&#39;application ou du dispositif d&#39;utilisateur qui génère une requête d&#39;utilisateur. le secret est adapté à un identifiant d&#39;utilisateur et à un identifiant de session spécifiques. le grand modèle de langage reçoit l&#39;instruction de répéter le secret dans chaque réponse. l&#39;agent de sécurité extrait la réponse du grand modèle de langage et vérifie le secret. lorsque le secret ne fait pas partie de la réponse, un message d&#39;erreur est transmis à l&#39;application d&#39;utilisateur au lieu de la réponse. "
US-12107885-B1,Prompt injection classifier using intermediate results,"An analysis engine receives data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model. An intermediate result of the GenAI model or a proxy of the GenAI model responsive to the prompt is obtained. The analysis engine, using a prompt injection classifier and the intermediate result, determines whether the prompt comprises or is indicative of malicious content or elicits malicious actions. Data characterizing the determination is provided to a consuming application or process. Related apparatus, systems, techniques and articles are also described.","What is claimed is: 
     
       1. A computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model, the GenAI model comprising a plurality of layers including an input layer, an output layer, and a plurality of intermediate layers positioned subsequent to the input layer and prior to the output layer; 
 capturing an intermediate result from at least one of the plurality of intermediate layers of the GenAI model; 
 determining, using a prompt injection classifier and based on the intermediate result, whether the prompt comprises malicious content or elicits malicious actions; and 
 providing data characterizing the determination to a consuming application or process to prevent the GenAI model from behaving in an undesired manner. 
 
     
     
       2. The method of  claim 1 , wherein the plurality of intermediate layers of the GenAI model comprise[s] a plurality of transformer layers and the intermediate result comprises activations in residual streams generated by one or more of the transformer layers. 
     
     
       3. The method of  claim 1 , wherein the GenAI model comprises a mixture of experts (MoE) model and the intermediate result comprises outputs from at least a subset of experts in the MoE model. 
     
     
       4. The method of  claim 1  further comprising:
 reducing a dimensionality of the intermediate result; and 
 wherein the prompt injection classifier uses the reduced dimensionality representation of the intermediate result when making the determination. 
 
     
     
       5. The method of  claim 1 , wherein the consuming application or process prevents the prompt from being input into the GenAI model upon a determination that the prompt comprises or elicits malicious content. 
     
     
       6. The method of  claim 1 , wherein the consuming application or process allows the prompt to be input into the GenAI model upon a determination that the prompt does not comprise or elicit malicious content. 
     
     
       7. The method of  claim 1 , wherein the consuming application or process flags the prompt as being malicious for quality assurance upon a determination that the prompt comprises or elicits malicious content. 
     
     
       8. The method of  claim 1 , wherein the consuming application or process modifies the prompt to be benign upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the GenAI model. 
     
     
       9. The method of  claim 1 , wherein the consuming application or process blocks an internet protocol (IP) address of a requester of the prompt upon a determination that the prompt comprises or elicits malicious content. 
     
     
       10. The method of  claim 1 , wherein the consuming application or process causes subsequent prompts from an internet protocol (IP) address of a requester of the prompt to be modified upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the GenAI model. 
     
     
       11. The method of  claim 1 , wherein the intermediate result is captured by a proxy of the GenAI model. 
     
     
       12. The method of  claim 11 , wherein the proxy is a quantized version of the GenAI model. 
     
     
       13. The method of  claim 1  further comprising: quantizing the GenAI model prior to capturing the intermediate result. 
     
     
       14. A computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model, the GenAI model comprising a plurality of layers including an input layer, an output layer, and a plurality of intermediate layers positioned subsequent to the input layer and prior to the output layer; 
 capturing an intermediate result from at least one of the plurality of intermediate layers of the GenAI model; 
 determining, using a prompt injection classifier and based on the intermediate result, a category for the prompt which is indicative of whether the prompt comprises or elicits malicious content; and 
 providing data characterizing the determination to a consuming application or process to prevent the GenAI model from behaving in an undesired manner. 
 
     
     
       15. The method of  claim 14 , wherein the plurality of intermediate layers of the GenAI model comprise[s] a plurality of transformer layers and the intermediate result comprises activations in residual streams generated by one or more the transformer layers. 
     
     
       16. The method of  claim 14 , wherein the GenAI model comprises a mixture of experts (MoE) model and the intermediate result comprises outputs from at least a subset of experts in the MoE model. 
     
     
       17. The method of  claim 14 , wherein the category specifies a threat severity for the prompt. 
     
     
       18. The method of  claim 17 , wherein the threat severity categories comprise one or more of: malicious, suspicious, unknown, or benign. 
     
     
       19. The method of  claim 14 , wherein the category specifies a type of prompt injection attack. 
     
     
       20. The method of  claim 19 , wherein the type of prompt injection attack comprises one or more of: a direct task deflection attack, a special case attack, a context continuation attack, a context termination attack, a syntactic transformation attack, an encryption attack, a text redirection attack. 
     
     
       21. The method of  claim 19 , wherein the prompt injection classifier comprises a plurality of different machine learning models, at least a portion of the different machine learning models being trained to categorize a different type of prompt injection attack. 
     
     
       22. The method of  claim 19  further comprising:
 initiating one or more remediation actions tailored to the specified type of prompt injection attack. 
 
     
     
       23. The method of  claim 14  further comprising:
 reducing a dimensionality of the intermediate result; and 
 wherein the prompt injection classifier uses the reduced dimensionality representation of the intermediate result when making the determination. 
 
     
     
       24. The method of  claim 14 , wherein the consuming application or process allows the prompt to be input into the GenAI model upon a determination that the prompt is of a category that does not comprise or elicit malicious content. 
     
     
       25. The method of  claim 14 , wherein the consuming application or process prevents the prompt from being input into the GenAI model upon a determination that the prompt is of a category that comprises or elicits malicious content. 
     
     
       26. The method of  claim 14 , wherein the consuming application or process flags the prompt as being malicious for quality assurance upon a determination that the prompt is of a category that comprises or elicits malicious content. 
     
     
       27. The method of  claim 11 , wherein the consuming application or process modifies the prompt to be benign upon a determination that the prompt is of a category that comprises or elicits malicious content and causes the modified prompt to be ingested by the GenAI model. 
     
     
       28. The method of  claim 11 , wherein the consuming application or process blocks an internet protocol (IP) address of a requester of the prompt upon a determination that the prompt is of a category that comprises or elicits malicious content. 
     
     
       29. The method of  claim 11 , wherein the consuming application or process causes subsequent prompts from an internet protocol (IP) address of a requester of the prompt to be modified upon a determination that the prompt is of a category that comprises or elicits malicious content and causes the modified prompt to be ingested by the GenAI model. 
     
     
       30. A computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model, the GenAI model comprising a plurality of layers including an input layer, an output layer, and a plurality of intermediate layers positioned subsequent to the input layer and prior to the output layer; 
 capturing an intermediate result from at least one of the plurality of intermediate layers of a quantized version of the GenAI model; 
 determining, using a prompt injection classifier and based on the intermediate result, whether the prompt comprises malicious content or elicits malicious actions; and 
 providing data characterizing the determination to a consuming application or process to prevent the GenAI model from behaving in an undesired manner.","HiddenLayer, Inc.","KAWASAKI, AMELIA | DAVIS, ANDREW",G06N3/045 | H04L63/1466 | H04L63/1416 | G06N20/00 | G06N20/00 | H04L63/1416 | H04L63/1466 | H04L63/1416 | G06N20/00 | H04L63/1466,G06N20/00 | H04L9/40,2024-10-01,20240426,20240426,US,US-202418648252-A,92899618,2024.0,"prompt injection classifier using intermediate results an analysis engine receives data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model. an intermediate result of the genai model or a proxy of the genai model responsive to the prompt is obtained. the analysis engine, using a prompt injection classifier and the intermediate result, determines whether the prompt comprises or is indicative of malicious content or elicits malicious actions. data characterizing the determination is provided to a consuming application or process. related apparatus, systems, techniques and articles are also described. what is claimed is: 
     
       1. a computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model, the genai model comprising a plurality of layers including an input layer, an output layer, and a plurality of intermediate layers positioned subsequent to the input layer and prior to the output layer; 
 capturing an intermediate result from at least one of the plurality of intermediate layers of the genai model; 
 determining, using a prompt injection classifier and based on the intermediate result, whether the prompt comprises malicious content or elicits malicious actions; and 
 providing data characterizing the determination to a consuming application or process to prevent the genai model from behaving in an undesired manner. 
 
     
     
       2. the method of  claim 1 , wherein the plurality of intermediate layers of the genai model comprise[s] a plurality of transformer layers and the intermediate result comprises activations in residual streams generated by one or more of the transformer layers. 
     
     
       3. the method of  claim 1 , wherein the genai model comprises a mixture of experts (moe) model and the intermediate result comprises outputs from at least a subset of experts in the moe model. 
     
     
       4. the method of  claim 1  further comprising:
 reducing a dimensionality of the intermediate result; and 
 wherein the prompt injection classifier uses the reduced dimensionality representation of the intermediate result when making the determination. 
 
     
     
       5. the method of  claim 1 , wherein the consuming application or process prevents the prompt from being input into the genai model upon a determination that the prompt comprises or elicits malicious content. 
     
     
       6. the method of  claim 1 , wherein the consuming application or process allows the prompt to be input into the genai model upon a determination that the prompt does not comprise or elicit malicious content. 
     
     
       7. the method of  claim 1 , wherein the consuming application or process flags the prompt as being malicious for quality assurance upon a determination that the prompt comprises or elicits malicious content. 
     
     
       8. the method of  claim 1 , wherein the consuming application or process modifies the prompt to be benign upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the genai model. 
     
     
       9. the method of  claim 1 , wherein the consuming application or process blocks an internet protocol (ip) address of a requester of the prompt upon a determination that the prompt comprises or elicits malicious content. 
     
     
       10. the method of  claim 1 , wherein the consuming application or process causes subsequent prompts from an internet protocol (ip) address of a requester of the prompt to be modified upon a determination that the prompt comprises or elicits malicious content and causes the modified prompt to be ingested by the genai model. 
     
     
       11. the method of  claim 1 , wherein the intermediate result is captured by a proxy of the genai model. 
     
     
       12. the method of  claim 11 , wherein the proxy is a quantized version of the genai model. 
     
     
       13. the method of  claim 1  further comprising: quantizing the genai model prior to capturing the intermediate result. 
     
     
       14. a computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model, the genai model comprising a plurality of layers including an input layer, an output layer, and a plurality of intermediate layers positioned subsequent to the input layer and prior to the output layer; 
 capturing an intermediate result from at least one of the plurality of intermediate layers of the genai model; 
 determining, using a prompt injection classifier and based on the intermediate result, a category for the prompt which is indicative of whether the prompt comprises or elicits malicious content; and 
 providing data characterizing the determination to a consuming application or process to prevent the genai model from behaving in an undesired manner. 
 
     
     
       15. the method of  claim 14 , wherein the plurality of intermediate layers of the genai model comprise[s] a plurality of transformer layers and the intermediate result comprises activations in residual streams generated by one or more the transformer layers. 
     
     
       16. the method of  claim 14 , wherein the genai model comprises a mixture of experts (moe) model and the intermediate result comprises outputs from at least a subset of experts in the moe model. 
     
     
       17. the method of  claim 14 , wherein the category specifies a threat severity for the prompt. 
     
     
       18. the method of  claim 17 , wherein the threat severity categories comprise one or more of: malicious, suspicious, unknown, or benign. 
     
     
       19. the method of  claim 14 , wherein the category specifies a type of prompt injection attack. 
     
     
       20. the method of  claim 19 , wherein the type of prompt injection attack comprises one or more of: a direct task deflection attack, a special case attack, a context continuation attack, a context termination attack, a syntactic transformation attack, an encryption attack, a text redirection attack. 
     
     
       21. the method of  claim 19 , wherein the prompt injection classifier comprises a plurality of different machine learning models, at least a portion of the different machine learning models being trained to categorize a different type of prompt injection attack. 
     
     
       22. the method of  claim 19  further comprising:
 initiating one or more remediation actions tailored to the specified type of prompt injection attack. 
 
     
     
       23. the method of  claim 14  further comprising:
 reducing a dimensionality of the intermediate result; and 
 wherein the prompt injection classifier uses the reduced dimensionality representation of the intermediate result when making the determination. 
 
     
     
       24. the method of  claim 14 , wherein the consuming application or process allows the prompt to be input into the genai model upon a determination that the prompt is of a category that does not comprise or elicit malicious content. 
     
     
       25. the method of  claim 14 , wherein the consuming application or process prevents the prompt from being input into the genai model upon a determination that the prompt is of a category that comprises or elicits malicious content. 
     
     
       26. the method of  claim 14 , wherein the consuming application or process flags the prompt as being malicious for quality assurance upon a determination that the prompt is of a category that comprises or elicits malicious content. 
     
     
       27. the method of  claim 11 , wherein the consuming application or process modifies the prompt to be benign upon a determination that the prompt is of a category that comprises or elicits malicious content and causes the modified prompt to be ingested by the genai model. 
     
     
       28. the method of  claim 11 , wherein the consuming application or process blocks an internet protocol (ip) address of a requester of the prompt upon a determination that the prompt is of a category that comprises or elicits malicious content. 
     
     
       29. the method of  claim 11 , wherein the consuming application or process causes subsequent prompts from an internet protocol (ip) address of a requester of the prompt to be modified upon a determination that the prompt is of a category that comprises or elicits malicious content and causes the modified prompt to be ingested by the genai model. 
     
     
       30. a computer-implemented method comprising:
 receiving data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model, the genai model comprising a plurality of layers including an input layer, an output layer, and a plurality of intermediate layers positioned subsequent to the input layer and prior to the output layer; 
 capturing an intermediate result from at least one of the plurality of intermediate layers of a quantized version of the genai model; 
 determining, using a prompt injection classifier and based on the intermediate result, whether the prompt comprises malicious content or elicits malicious actions; and 
 providing data characterizing the determination to a consuming application or process to prevent the genai model from behaving in an undesired manner."
US-2024354379-A1,Guardrail machine learning model for automated software,"In one aspect, a method of guarding an automated software includes generating, by a first language model, a training set of rule-violating data, generating, by the first language model, a training set of contrastive examples by altering the rule-violating data into non-violating data, training a guardrail machine learning model using the generated training sets, generating, with an automated software, an output based on a user input, monitoring with the trained guardrail machine learn model whether the generated output violates a rule, and preventing the automated software from transmitting to the user the generated output that violates a rule.","What is claimed is: 
     
         1 . A method of guarding automated software by a guardrail machine learning model, comprising:
 generating, by a first language model, a training set of rule-violating data;   generating, by the first language model, a training set of contrastive examples by altering the rule-violating data into non-violating data;   training the guardrail machine learning model using the generated training sets;   generating, with the automated software, an output based on a user input;   monitoring with the trained guardrail machine learn model whether the generated output violates a rule; and   preventing the automated software from transmitting to the user the generated output that violates a rule.   
     
     
         2 . The method of  claim 1 , further comprising modifying the generated output that violates the rule and transmitting the modified output to the user. 
     
     
         3 . The method of  claim 1 , wherein the guardrail machine learning model is smaller than the first language model. 
     
     
         4 . The method of  claim 1 , wherein the first language model, trained guardrail machine learning model and the automated software are run on different servers. 
     
     
         5 . The method of  claim 1 , further comprising:
 generating, by the first language model, a training set of non-violating data and wherein the training trains the guardrail machine learning model on the training set of non-violating data.   
     
     
         6 . The method of  claim 1 , further comprising:
 generating, with the first language model, a scenario and wherein the generating the training sets is based on the generated scenario.   
     
     
         7 . The method of  claim 1 , wherein the trained guardrail machine learning model has a lower latency than the first language model. 
     
     
         8 . The method of  claim 1 , further comprising:
 generating, with the first language model, a set of rules for a domain.   
     
     
         9 . The method of  claim 8 , wherein the set of rules is non-overlapping. 
     
     
         10 . A non-transitory computer-readable storage medium, the computer-readable storage medium including instructions that when executed by a computer, cause the computer to:
 generate, by a first language model, a training set of rule-violating data;   generate, by the first language model, a training set of contrastive examples by altering the rule-violating data into non-violating data;   train a guardrail machine learning model using the generated training sets;   generate, with an automated software, an output based on a user input;   monitor with the trained guardrail machine learn model whether the generated output violates a rule; and   prevent the automated software from transmitting to the user the generated output that violates a rule.   
     
     
         11 . A computing apparatus comprising:
 a processor; and   a non-transitory memory storing instructions that, when executed by the processor, configure the apparatus to:   generate, by a first language model, a training set of rule-violating data;   generate, by the first language model, a training set of contrastive examples by altering the rule-violating data into non-violating data;   train a guardrail machine learning model using the generated training sets;   generate, with an automated software, an output based on a user input;   monitor with the trained guardrail machine learn model whether the generated output violates a rule; and   prevent the automated software from transmitting to the user the generated output that violates a rule.   
     
     
         12 . The computing apparatus of  claim 11 , wherein the instructions further configure the apparatus to modify the generated output that violates the rule and transmitting the modified output to the user. 
     
     
         13 . The computing apparatus of  claim 11 , wherein the guardrail machine learning model is smaller than the first language model. 
     
     
         14 . The computing apparatus of  claim 11 , wherein the first language model, trained guardrail machine learning model and the automated software are run on different servers. 
     
     
         15 . The computing apparatus of  claim 11 , wherein the instructions further configure the apparatus to:
 generate, by the first language model, a training set of non-violating data and wherein the training trains the guardrail machine learning model on the training set of non-violating data.   
     
     
         16 . The computing apparatus of  claim 11 , wherein the instructions further configure the apparatus to:
 generate, with the first language model, a scenario and wherein the generating the training sets is based on the generated scenario.   
     
     
         17 . The computing apparatus of  claim 11 , wherein the trained guardrail machine learn model has a lower latency than the first language model. 
     
     
         18 . The computing apparatus of  claim 11 , wherein the instructions further configure the apparatus to:
 generate, with the first language model, a set of rules for a domain.   
     
     
         19 . The computing apparatus of  claim 18 , wherein the set of rules is non-overlapping. 
     
     
         20 . The computing apparatus of  claim 11 , wherein the first language model and trained guardrail model are generative pre-trained transformers.","Curai, Inc.",,,,20241024,20240417,20230418,US,US-202418638585-A,93121386,,
US-12130943-B1,Generative artificial intelligence model personally identifiable information detection and protection,"An analysis engine receives data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model. The analysis engine, using the received data, determines whether the prompt comprises personally identifiable information (PII) or elicits PII from the GenAI model. The analysis engine can use pattern recognition to identify PII entities in the prompt. Data characterizing the determination is provided to a consuming application or process. Related apparatus, systems, techniques and articles are also described.","What is claimed is: 
     
       1. A computer-implemented method comprising:
 receiving, from a proxy executing in a model environment, data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model executing in the model environment; 
 calling a remote service to identify, using each of a plurality of pattern recognition algorithms and a first machine learning model, personally identifiable information (PII) entities in the prompt, wherein each of the identified PII entities are classified as one of a plurality of PII entity types, 
 determining, by the remote service, whether aspects of the prompt comprise permissible or impermissible PII based on the PII entity type and context associated with the prompt; 
 providing data characterizing the determination of whether aspects of the prompt comprises permissible or impermissible PII to a consuming application or process, the consuming application or process taking remediation actions in connection with the aspects of the prompt that comprise impermissible PII while not modifying the aspects of the prompt that comprise permissible PII; 
 receiving an output of the GenAI model in response to the prompt, when it is determined that the prompt does not comprise impermissible PII; 
 determining, by the remote service, using the plurality of pattern recognition algorithms and a second machine learning model, whether the output comprises permissible PII or impermissible PII; 
 determining whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the GenAI model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; 
 allowing the output to be transmitted to a requesting user when it is determined that the output does not comprise impermissible PII and it is determined that the output does not indicate that the prompt contained or elicited malicious content or undesired behavior; 
 preventing the output from being transmitted to a requestor when it is determined that the output comprises impermissible PII; and 
 preventing the output from being transmitted to a requestor when it is determined that the output indicates that the prompt contained or elicited malicious content or undesired behavior. 
 
     
     
       2. The method of  claim 1  further comprising:
 classifying the identified PII entities as one of a plurality of entity types using at least one third machine learning model. 
 
     
     
       3. The method of  claim 2  further comprising:
 initiating at least one remediation action corresponding to the entity type to modify or block the prompt. 
 
     
     
       4. The method of  claim 1  wherein the GenAI model comprises a large language model. 
     
     
       5. The method of  claim 1 , wherein the consuming application or process allows the prompt to be input into the GenAI model upon a determination that the prompt does not comprise impermissible PII. 
     
     
       6. The method of  claim 1 , wherein the consuming application or process prevents the prompt from being input into the GenAI model upon a determination that the prompt comprises impermissible PII. 
     
     
       7. The method of  claim 1 , wherein the consuming application or process flags the prompt as comprising PII for quality assurance upon a determination that the prompt comprises impermissible PII. 
     
     
       8. The method of  claim 1 , wherein the consuming application or process modifies the prompt to remove or redact the impermissible PII upon a determination that the prompt comprises impermissible PII and causes the modified prompt to be ingested by the GenAI model. 
     
     
       9. The method of  claim 1  further comprising:
 determining, using a blocklist, whether the prompt comprises or elicits undesired behavior from the GenAI model. 
 
     
     
       10. The method of  claim 9  further comprising:
 preventing the prompt from being ingested by the GenAI model when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the GenAI model. 
 
     
     
       11. The method of  claim 10  further comprising:
 modifying the prompt to be benign when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the GenAI model; and 
 causing the modified prompt to be ingested by the GenAI model. 
 
     
     
       12. The method of  claim 1 , wherein natural language processing is used to identify and extract strings belonging to specific entity types likely to comprise PII. 
     
     
       13. The method of  claim 1  further comprising:
 causing the prompt to be ingested by the GenAI model after completion of the remediation actions. 
 
     
     
       14. The method of  claim 1 , wherein the first machine learning model is same as the second machine learning model. 
     
     
       15. The method of  claim 1 , wherein the first machine learning model is trained and configured to analyze prompts of the GenAI model and the second machine learning model is trained and configured to analyze outputs of the GenAI model. 
     
     
       16. A system comprising:
 at least one hardware processor; and 
 memory storing instructions which, when executed by the at least one hardware processor, result in operations comprising:
 receiving, from a proxy executing in a model environment, data characterizing a prompt for ingestion by a generative artificial intelligence (GenAI) model executing in the model environment; 
 calling a remote service to identify, using each of a plurality of pattern recognition algorithms and at least one machine learning model, personally identifiable information (PII) entities in the prompt, wherein each of the identified PII entities are classified as one of a plurality of PII entity types, 
 determining, by the remote service, whether aspects of the prompt comprise permissible PII or impermissible PII based on the PII entity type and context associated with the prompt; 
 providing data characterizing the determination of whether aspects of the prompt comprises permissible or impermissible PII to a consuming application or process, the consuming application or process taking remediation actions in connection with the aspects of the prompt that comprise impermissible PII while not modifying the aspects of the prompt that comprise permissible PII; 
 receiving an output of the GenAI model in response to the prompt, when it is determined that the prompt does not comprise impermissible PII; 
 determining, by the remote service, using the plurality of pattern recognition algorithms and the at least one machine learning model, whether the output comprises permissible PII or impermissible PII; 
 determining whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the GenAI model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; 
 allowing the output to be transmitted to a requesting user when it is determined that the output does not comprise impermissible PII and it is determined that the output does not indicate that the prompt contained or elicited malicious content or undesired behavior; 
 preventing the output from being transmitted to a requestor when it is determined that the output comprises impermissible PII; and 
 preventing the output from being transmitted to a requestor when it is determined that the output indicates that the prompt contained or elicited malicious content or undesired behavior. 
 
 
     
     
       17. The system of  claim 16 , wherein the operation further comprise:
 classifying the identified PII entities as one of a plurality of entity types using at least one machine learning model. 
 
     
     
       18. The system of  claim 17 , wherein the operations further comprise:
 initiating at least one remediation action corresponding to the entity type to modify or block the prompt. 
 
     
     
       19. The system of  claim 16  wherein the GenAI model comprises a large language model. 
     
     
       20. The system of  claim 16 , wherein the consuming application or process allows the prompt to be input into the GenAI model upon a determination that the prompt does not comprise impermissible PII. 
     
     
       21. The system of  claim 16 , wherein the consuming application or process prevents the prompt from being input into the GenAI model upon a determination that the prompt comprises impermissible PII. 
     
     
       22. The system of  claim 16 , wherein the consuming application or process flags the prompt as comprising PII for quality assurance upon a determination that the prompt comprises impermissible PII. 
     
     
       23. The system of  claim 16 , wherein the consuming application or process modifies the prompt to remove or redact the impermissible PII upon a determination that the prompt comprises impermissible PII and causes the modified prompt to be ingested by the GenAI model. 
     
     
       24. The system of  claim 16 , wherein the operations further comprise:
 determining, using a blocklist, whether the prompt comprises or elicits undesired behavior from the GenAI model. 
 
     
     
       25. The system of  claim 24 , wherein the operations further comprise:
 preventing the prompt from being ingested by the GenAI model when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the GenAI model. 
 
     
     
       26. The system of  claim 25 , wherein the operations further comprise:
 modifying the prompt to be benign when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the GenAI model; and 
 causing the modified prompt to be ingested by the GenAI model. 
 
     
     
       27. The system of  claim 16 , wherein natural language processing is used to identify and extract strings belonging to specific entity types likely to comprise PII. 
     
     
       28. A system comprising:
 a plurality of first computing devices each having at least one hardware processor and forming a model environment and executing a generative artificial intelligence (GenAI model), the model environment executing a proxy which redirects inputs and outputs of the GenAI model to a monitoring environment; and 
 a plurality of second computing devices each having at least one hardware processor and forming the monitoring environment, the monitoring environment executing an analysis engine and a remediation engine; 
 wherein the analysis engine:
 receives data which characterizes a prompt for ingestion by the GenAI model model from the proxy; 
 calls a remote service to identify, using each of a plurality of pattern recognition algorithms and a first machine learning model, personally identifiable information (PII) entities in the prompt, wherein each of the identified PII entities are classified as one of a plurality of PII entity types, the remote service determining whether aspects of the prompt comprise permissible PII or impermissible PII based on the PII entity type and context associated with the prompt; 
 provides data characterizing the determination of whether aspects of the prompt comprises permissible or impermissible PII to a consuming application or process, the consuming application or process causing the remediation engine to remediation actions in connection with the aspects of the prompt that comprise impermissible PII while not modifying the aspects of the prompt that comprise permissible PII; 
 receives an output of the GenAI model in response to the prompt, when it is determined that the prompt does not comprise impermissible PII; 
 calls the remote service to determine, using the plurality of pattern recognition algorithms and a second machine learning model, whether the output comprises permissible PII or impermissible PII; 
 determines whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the GenAI model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; 
 allows the output to be transmitted to a requesting user when it is determined that the output does not comprise impermissible PII and it is determined that the output does not indicate that the prompt contained or elicited malicious content or undesired behavior; 
 instructs the remediation engine to prevent the output from being transmitted to a requestor when it is determined that the output comprises impermissible PII; and 
 instructs the remediation engine to prevent the output from being transmitted to a requestor when it is determined that the output indicates that the prompt contained or elicited malicious content or undesired behavior.","HiddenLayer, Inc.","Burns, Tanner | CAPPEL, KWESI | YEUNG, KENNETH",G06F21/6245 | G06F21/6245 | G06F21/6245,G06F21/62,2024-10-29,20240329,20240329,US,US-202418621751-A,93217151,2024.0,"generative artificial intelligence model personally identifiable information detection and protection an analysis engine receives data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model. the analysis engine, using the received data, determines whether the prompt comprises personally identifiable information (pii) or elicits pii from the genai model. the analysis engine can use pattern recognition to identify pii entities in the prompt. data characterizing the determination is provided to a consuming application or process. related apparatus, systems, techniques and articles are also described. what is claimed is: 
     
       1. a computer-implemented method comprising:
 receiving, from a proxy executing in a model environment, data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model executing in the model environment; 
 calling a remote service to identify, using each of a plurality of pattern recognition algorithms and a first machine learning model, personally identifiable information (pii) entities in the prompt, wherein each of the identified pii entities are classified as one of a plurality of pii entity types, 
 determining, by the remote service, whether aspects of the prompt comprise permissible or impermissible pii based on the pii entity type and context associated with the prompt; 
 providing data characterizing the determination of whether aspects of the prompt comprises permissible or impermissible pii to a consuming application or process, the consuming application or process taking remediation actions in connection with the aspects of the prompt that comprise impermissible pii while not modifying the aspects of the prompt that comprise permissible pii; 
 receiving an output of the genai model in response to the prompt, when it is determined that the prompt does not comprise impermissible pii; 
 determining, by the remote service, using the plurality of pattern recognition algorithms and a second machine learning model, whether the output comprises permissible pii or impermissible pii; 
 determining whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the genai model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; 
 allowing the output to be transmitted to a requesting user when it is determined that the output does not comprise impermissible pii and it is determined that the output does not indicate that the prompt contained or elicited malicious content or undesired behavior; 
 preventing the output from being transmitted to a requestor when it is determined that the output comprises impermissible pii; and 
 preventing the output from being transmitted to a requestor when it is determined that the output indicates that the prompt contained or elicited malicious content or undesired behavior. 
 
     
     
       2. the method of  claim 1  further comprising:
 classifying the identified pii entities as one of a plurality of entity types using at least one third machine learning model. 
 
     
     
       3. the method of  claim 2  further comprising:
 initiating at least one remediation action corresponding to the entity type to modify or block the prompt. 
 
     
     
       4. the method of  claim 1  wherein the genai model comprises a large language model. 
     
     
       5. the method of  claim 1 , wherein the consuming application or process allows the prompt to be input into the genai model upon a determination that the prompt does not comprise impermissible pii. 
     
     
       6. the method of  claim 1 , wherein the consuming application or process prevents the prompt from being input into the genai model upon a determination that the prompt comprises impermissible pii. 
     
     
       7. the method of  claim 1 , wherein the consuming application or process flags the prompt as comprising pii for quality assurance upon a determination that the prompt comprises impermissible pii. 
     
     
       8. the method of  claim 1 , wherein the consuming application or process modifies the prompt to remove or redact the impermissible pii upon a determination that the prompt comprises impermissible pii and causes the modified prompt to be ingested by the genai model. 
     
     
       9. the method of  claim 1  further comprising:
 determining, using a blocklist, whether the prompt comprises or elicits undesired behavior from the genai model. 
 
     
     
       10. the method of  claim 9  further comprising:
 preventing the prompt from being ingested by the genai model when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the genai model. 
 
     
     
       11. the method of  claim 10  further comprising:
 modifying the prompt to be benign when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the genai model; and 
 causing the modified prompt to be ingested by the genai model. 
 
     
     
       12. the method of  claim 1 , wherein natural language processing is used to identify and extract strings belonging to specific entity types likely to comprise pii. 
     
     
       13. the method of  claim 1  further comprising:
 causing the prompt to be ingested by the genai model after completion of the remediation actions. 
 
     
     
       14. the method of  claim 1 , wherein the first machine learning model is same as the second machine learning model. 
     
     
       15. the method of  claim 1 , wherein the first machine learning model is trained and configured to analyze prompts of the genai model and the second machine learning model is trained and configured to analyze outputs of the genai model. 
     
     
       16. a system comprising:
 at least one hardware processor; and 
 memory storing instructions which, when executed by the at least one hardware processor, result in operations comprising:
 receiving, from a proxy executing in a model environment, data characterizing a prompt for ingestion by a generative artificial intelligence (genai) model executing in the model environment; 
 calling a remote service to identify, using each of a plurality of pattern recognition algorithms and at least one machine learning model, personally identifiable information (pii) entities in the prompt, wherein each of the identified pii entities are classified as one of a plurality of pii entity types, 
 determining, by the remote service, whether aspects of the prompt comprise permissible pii or impermissible pii based on the pii entity type and context associated with the prompt; 
 providing data characterizing the determination of whether aspects of the prompt comprises permissible or impermissible pii to a consuming application or process, the consuming application or process taking remediation actions in connection with the aspects of the prompt that comprise impermissible pii while not modifying the aspects of the prompt that comprise permissible pii; 
 receiving an output of the genai model in response to the prompt, when it is determined that the prompt does not comprise impermissible pii; 
 determining, by the remote service, using the plurality of pattern recognition algorithms and the at least one machine learning model, whether the output comprises permissible pii or impermissible pii; 
 determining whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the genai model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; 
 allowing the output to be transmitted to a requesting user when it is determined that the output does not comprise impermissible pii and it is determined that the output does not indicate that the prompt contained or elicited malicious content or undesired behavior; 
 preventing the output from being transmitted to a requestor when it is determined that the output comprises impermissible pii; and 
 preventing the output from being transmitted to a requestor when it is determined that the output indicates that the prompt contained or elicited malicious content or undesired behavior. 
 
 
     
     
       17. the system of  claim 16 , wherein the operation further comprise:
 classifying the identified pii entities as one of a plurality of entity types using at least one machine learning model. 
 
     
     
       18. the system of  claim 17 , wherein the operations further comprise:
 initiating at least one remediation action corresponding to the entity type to modify or block the prompt. 
 
     
     
       19. the system of  claim 16  wherein the genai model comprises a large language model. 
     
     
       20. the system of  claim 16 , wherein the consuming application or process allows the prompt to be input into the genai model upon a determination that the prompt does not comprise impermissible pii. 
     
     
       21. the system of  claim 16 , wherein the consuming application or process prevents the prompt from being input into the genai model upon a determination that the prompt comprises impermissible pii. 
     
     
       22. the system of  claim 16 , wherein the consuming application or process flags the prompt as comprising pii for quality assurance upon a determination that the prompt comprises impermissible pii. 
     
     
       23. the system of  claim 16 , wherein the consuming application or process modifies the prompt to remove or redact the impermissible pii upon a determination that the prompt comprises impermissible pii and causes the modified prompt to be ingested by the genai model. 
     
     
       24. the system of  claim 16 , wherein the operations further comprise:
 determining, using a blocklist, whether the prompt comprises or elicits undesired behavior from the genai model. 
 
     
     
       25. the system of  claim 24 , wherein the operations further comprise:
 preventing the prompt from being ingested by the genai model when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the genai model. 
 
     
     
       26. the system of  claim 25 , wherein the operations further comprise:
 modifying the prompt to be benign when it is determined using the blocklist that the prompt comprises or elicits undesired behavior from the genai model; and 
 causing the modified prompt to be ingested by the genai model. 
 
     
     
       27. the system of  claim 16 , wherein natural language processing is used to identify and extract strings belonging to specific entity types likely to comprise pii. 
     
     
       28. a system comprising:
 a plurality of first computing devices each having at least one hardware processor and forming a model environment and executing a generative artificial intelligence (genai model), the model environment executing a proxy which redirects inputs and outputs of the genai model to a monitoring environment; and 
 a plurality of second computing devices each having at least one hardware processor and forming the monitoring environment, the monitoring environment executing an analysis engine and a remediation engine; 
 wherein the analysis engine:
 receives data which characterizes a prompt for ingestion by the genai model model from the proxy; 
 calls a remote service to identify, using each of a plurality of pattern recognition algorithms and a first machine learning model, personally identifiable information (pii) entities in the prompt, wherein each of the identified pii entities are classified as one of a plurality of pii entity types, the remote service determining whether aspects of the prompt comprise permissible pii or impermissible pii based on the pii entity type and context associated with the prompt; 
 provides data characterizing the determination of whether aspects of the prompt comprises permissible or impermissible pii to a consuming application or process, the consuming application or process causing the remediation engine to remediation actions in connection with the aspects of the prompt that comprise impermissible pii while not modifying the aspects of the prompt that comprise permissible pii; 
 receives an output of the genai model in response to the prompt, when it is determined that the prompt does not comprise impermissible pii; 
 calls the remote service to determine, using the plurality of pattern recognition algorithms and a second machine learning model, whether the output comprises permissible pii or impermissible pii; 
 determines whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the genai model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; 
 allows the output to be transmitted to a requesting user when it is determined that the output does not comprise impermissible pii and it is determined that the output does not indicate that the prompt contained or elicited malicious content or undesired behavior; 
 instructs the remediation engine to prevent the output from being transmitted to a requestor when it is determined that the output comprises impermissible pii; and 
 instructs the remediation engine to prevent the output from being transmitted to a requestor when it is determined that the output indicates that the prompt contained or elicited malicious content or undesired behavior."
US-11995180-B1,Generative artificial intelligence model protection using output blocklist,"The inputs and/or outputs of a generative artificial intelligence model are monitored to determine whether they contain or otherwise elicit undesired behavior by the model such as bypassing security measures, leaking sensitive information, or generating or consuming malicious content. This determination can be used to selectively trigger remediation processes to protect the model from malicious actions. Related apparatus, systems, techniques and articles are also described.","What is claimed is: 
     
       1. A computer-implemented method comprising:
 receiving, by an analysis engine, data characterizing an output of a generative artificial intelligence (GenAI) model responsive to a prompt from a requesting client device; 
 determining, by the analysis engine using the received data, whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the GenAI model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; and 
 providing data characterizing the determination to a consuming application or process which prevents the output from being transmitted to the requesting client device upon a determination that the output indicates that the prompt comprises malicious content. 
 
     
     
       2. The method of  claim 1  further comprising:
 tokenizing the data characterizing the output to result in a plurality of tokens; and 
 wherein the similarity analysis compares the blocklist to the plurality of tokens. 
 
     
     
       3. The method of  claim 1  further comprising:
 vectorizing the data characterizing the output to result in one or more vectors; 
 generating one or more embeddings based on the one or more vectors, the embeddings having a lower dimensionality than the one or more vectors; and 
 wherein the similarity analysis compares the blocklist to the generated one or more embeddings. 
 
     
     
       4. The method of  claim 1 , wherein the similarity analysis comprises an N-grams similarity analysis. 
     
     
       5. The method of  claim 4  further comprising:
 generating the blocklist by deriving a plurality of N-grams from the corpus of machine learning model outputs responsive to malicious prompts. 
 
     
     
       6. The method of  claim 1 , wherein the similarity analysis comprises a semantic analysis in which distance measurements indicative of similarity are generated based on a likeness of meaning of the received data to the blocklist. 
     
     
       7. The method of  claim 1 , wherein the blocklist is ordered according to frequency and the similarity analysis terminates upon a similarity match above a pre-determined threshold. 
     
     
       8. The method of  claim 1 , wherein the GenAI model comprises a large language model. 
     
     
       9. The method of  claim 1 , wherein the consuming application or process allows the output to be transmitted to the requesting client device upon a determination that the output indicates that the prompt does not comprise malicious content. 
     
     
       10. The method of  claim 1 , wherein the consuming application or process flags the output as containing sensitive information for quality assurance upon a determination that the output indicates that the prompt comprises malicious content. 
     
     
       11. A computer-implemented method comprising:
 receiving, by an analysis engine, data characterizing an output of a generative artificial intelligence (GenAI) model responsive to a prompt or query from a requesting client device; 
 determining, by the analysis engine using the received data, whether the output comprises sensitive information based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to prompts resulting in undesired behavior by the GenAI model; and 
 providing data characterizing the determination to a consuming application or process which prevents the output from being transmitted to a requesting client device upon a determination that the prompt comprises sensitive information. 
 
     
     
       12. The method of  claim 11  further comprising:
 tokenizing the data characterizing the output to result in a plurality of tokens; and 
 wherein the similarity analysis compares the blocklist to the plurality of tokens. 
 
     
     
       13. The method of  claim 11  further comprising:
 vectorizing the data characterizing the output to result in one or more vectors; 
 generating one or more embeddings based on the one or more vectors, the embeddings having a lower dimensionality than the one or more vectors; and 
 wherein the similarity analysis compares the blocklist to the generated one or more embeddings. 
 
     
     
       14. The method of  claim 11 , wherein the similarity analysis comprises an N-grams similarity analysis. 
     
     
       15. The method of  claim 14  further comprising:
 generating the blocklist by deriving a plurality of N-grams from the corpus of machine learning model outputs responsive to prompts resulting in undesired behavior by the GenAI model. 
 
     
     
       16. The method of  claim 11 , wherein the similarity analysis comprises a semantic analysis in which distance measurements indicative of similarity are generated based on a likeness of meaning of the received data to the blocklist. 
     
     
       17. The method of  claim 11 , wherein the blocklist is ordered according to frequency and the similarity analysis terminates upon a similarity match above a pre-determined threshold. 
     
     
       18. The method of  claim 11 , wherein the GenAI model comprises a large language model. 
     
     
       19. The method of  claim 11 , wherein the consuming application or process allows the output to be transmitted to the requesting client device upon a determination that the prompt does not comprise sensitive information. 
     
     
       20. The method of  claim 11 , wherein the consuming application or process flags the output as containing sensitive information for quality assurance upon a determination that the output comprises sensitive information. 
     
     
       21. A system comprising:
 at least one data processor; and 
 non-transitory memory storing instructions which, when executed by the at least one data processor, result in operations comprising:
 receiving data characterizing an output of a generative artificial intelligence (GenAI) model responsive to a prompt from a requesting client device; 
 determining, using the received data, whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the GenAI model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; and 
 providing data characterizing the determination to a consuming application or process which prevents the output from being transmitted to the requesting client device upon a determination that the output indicates that the prompt comprises malicious content. 
 
 
     
     
       22. The system of  claim 21 , wherein the similarity analysis comprises an N-grams similarity analysis. 
     
     
       23. The system of  claim 22 , wherein the operations further comprise:
 generating the blocklist by deriving a plurality of N-grams from the corpus of machine learning model outputs responsive to malicious prompts. 
 
     
     
       24. The system of  claim 22 , wherein the similarity analysis comprises a semantic analysis in which distance measurements indicative of similarity are generated based on a likeness of meaning of the received data to the blocklist.","HiddenLayer, Inc.","CAPPEL, KWESI | Burns, Tanner | YEUNG, KENNETH",G06F21/55 | G06F21/55 | G06F21/55,G06F21/55,2024-05-28,20240131,20240131,US,US-202418429255-A,91196840,2024.0,"generative artificial intelligence model protection using output blocklist the inputs and/or outputs of a generative artificial intelligence model are monitored to determine whether they contain or otherwise elicit undesired behavior by the model such as bypassing security measures, leaking sensitive information, or generating or consuming malicious content. this determination can be used to selectively trigger remediation processes to protect the model from malicious actions. related apparatus, systems, techniques and articles are also described. what is claimed is: 
     
       1. a computer-implemented method comprising:
 receiving, by an analysis engine, data characterizing an output of a generative artificial intelligence (genai) model responsive to a prompt from a requesting client device; 
 determining, by the analysis engine using the received data, whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the genai model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; and 
 providing data characterizing the determination to a consuming application or process which prevents the output from being transmitted to the requesting client device upon a determination that the output indicates that the prompt comprises malicious content. 
 
     
     
       2. the method of  claim 1  further comprising:
 tokenizing the data characterizing the output to result in a plurality of tokens; and 
 wherein the similarity analysis compares the blocklist to the plurality of tokens. 
 
     
     
       3. the method of  claim 1  further comprising:
 vectorizing the data characterizing the output to result in one or more vectors; 
 generating one or more embeddings based on the one or more vectors, the embeddings having a lower dimensionality than the one or more vectors; and 
 wherein the similarity analysis compares the blocklist to the generated one or more embeddings. 
 
     
     
       4. the method of  claim 1 , wherein the similarity analysis comprises an n-grams similarity analysis. 
     
     
       5. the method of  claim 4  further comprising:
 generating the blocklist by deriving a plurality of n-grams from the corpus of machine learning model outputs responsive to malicious prompts. 
 
     
     
       6. the method of  claim 1 , wherein the similarity analysis comprises a semantic analysis in which distance measurements indicative of similarity are generated based on a likeness of meaning of the received data to the blocklist. 
     
     
       7. the method of  claim 1 , wherein the blocklist is ordered according to frequency and the similarity analysis terminates upon a similarity match above a pre-determined threshold. 
     
     
       8. the method of  claim 1 , wherein the genai model comprises a large language model. 
     
     
       9. the method of  claim 1 , wherein the consuming application or process allows the output to be transmitted to the requesting client device upon a determination that the output indicates that the prompt does not comprise malicious content. 
     
     
       10. the method of  claim 1 , wherein the consuming application or process flags the output as containing sensitive information for quality assurance upon a determination that the output indicates that the prompt comprises malicious content. 
     
     
       11. a computer-implemented method comprising:
 receiving, by an analysis engine, data characterizing an output of a generative artificial intelligence (genai) model responsive to a prompt or query from a requesting client device; 
 determining, by the analysis engine using the received data, whether the output comprises sensitive information based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to prompts resulting in undesired behavior by the genai model; and 
 providing data characterizing the determination to a consuming application or process which prevents the output from being transmitted to a requesting client device upon a determination that the prompt comprises sensitive information. 
 
     
     
       12. the method of  claim 11  further comprising:
 tokenizing the data characterizing the output to result in a plurality of tokens; and 
 wherein the similarity analysis compares the blocklist to the plurality of tokens. 
 
     
     
       13. the method of  claim 11  further comprising:
 vectorizing the data characterizing the output to result in one or more vectors; 
 generating one or more embeddings based on the one or more vectors, the embeddings having a lower dimensionality than the one or more vectors; and 
 wherein the similarity analysis compares the blocklist to the generated one or more embeddings. 
 
     
     
       14. the method of  claim 11 , wherein the similarity analysis comprises an n-grams similarity analysis. 
     
     
       15. the method of  claim 14  further comprising:
 generating the blocklist by deriving a plurality of n-grams from the corpus of machine learning model outputs responsive to prompts resulting in undesired behavior by the genai model. 
 
     
     
       16. the method of  claim 11 , wherein the similarity analysis comprises a semantic analysis in which distance measurements indicative of similarity are generated based on a likeness of meaning of the received data to the blocklist. 
     
     
       17. the method of  claim 11 , wherein the blocklist is ordered according to frequency and the similarity analysis terminates upon a similarity match above a pre-determined threshold. 
     
     
       18. the method of  claim 11 , wherein the genai model comprises a large language model. 
     
     
       19. the method of  claim 11 , wherein the consuming application or process allows the output to be transmitted to the requesting client device upon a determination that the prompt does not comprise sensitive information. 
     
     
       20. the method of  claim 11 , wherein the consuming application or process flags the output as containing sensitive information for quality assurance upon a determination that the output comprises sensitive information. 
     
     
       21. a system comprising:
 at least one data processor; and 
 non-transitory memory storing instructions which, when executed by the at least one data processor, result in operations comprising:
 receiving data characterizing an output of a generative artificial intelligence (genai) model responsive to a prompt from a requesting client device; 
 determining, using the received data, whether the output indicates that the prompt contained or elicited malicious content or undesired behavior by the genai model based on a similarity analysis between a blocklist and the received data, the blocklist being derived from a corpus of machine learning model outputs responsive to malicious prompts; and 
 providing data characterizing the determination to a consuming application or process which prevents the output from being transmitted to the requesting client device upon a determination that the output indicates that the prompt comprises malicious content. 
 
 
     
     
       22. the system of  claim 21 , wherein the similarity analysis comprises an n-grams similarity analysis. 
     
     
       23. the system of  claim 22 , wherein the operations further comprise:
 generating the blocklist by deriving a plurality of n-grams from the corpus of machine learning model outputs responsive to malicious prompts. 
 
     
     
       24. the system of  claim 22 , wherein the similarity analysis comprises a semantic analysis in which distance measurements indicative of similarity are generated based on a likeness of meaning of the received data to the blocklist."
US-12061970-B1,Systems and methods of large language model driven orchestration of task-specific machine learning software agents,"Systems and methods of the present disclosure may receive, from a user computing device, a user-provided data record query including a natural language request for information associated with one or more data sources. User persona attributes of the user may be determined, such as a user role or security parameters or both. Based on the user persona attributes a context query may be generated to obtain context attributes associated with the user-provided query. The natural language request and the context attributes are input into the model orchestration large language model (LLM) to output instructions to machine learning (ML) agents based on the context attributes. The ML agents output responses associated with the user-provided data record query based on the instructions, and the responses are input into the model orchestration LLM to output to the user computing device a natural language response based on the context attributes.","What is claimed is: 
     
       1. A method comprising:
 receiving, by at least one processor from at least one user computing device associated with a user, a user-provided data record query comprising a natural language request to perform at least one action with at least one data record; 
 determining, by the at least one processor, a user profile associated with the user;
 wherein the user profile comprises user persona attributes; 
 wherein the user persona attributes comprise at least one of:
 a user role within an organization, or 
 at least one security parameter associated with the user; 
 
 
 generating, by the at least one processor, based on the user persona attributes of the user profile, at least one context query to at least one data source so as to obtain at least one context attribute representative of additional context specific to the user associated with the user-provided data record query; 
 modifying, by the at least one processor, a model orchestration large language model runtime of the model orchestration large language model based at least in part on the at least one context attribute so as to customize the context of the model orchestration large language model for the user persona attributes; 
 inputting, by the at least one processor, the natural language request of the data record query as a data record query prompt into the model orchestration large language model to output at least one instruction to at least one data record processing machine learning agent of a plurality of data record processing machine learning agents based at least in part on trained parameters of the model orchestration large language model and the at least one context attribute; 
 inputting, by the at least one processor, the at least one instruction into the at least one data record processing machine learning agent to output at least one response; 
 inputting, by the at least one processor, the at least one response as a response prompt into the model orchestration large language model to output to the user computing device at least one natural language response representative of the at least one action based at least in part on trained parameters of the model orchestration large language model and the at least one context attribute; and 
 causing to display, by the at least one processor, the at least one natural language response in a graphical user interface (GUI) rendered on the user computing device. 
 
     
     
       2. The method of  claim 1 , wherein the plurality of data record processing machine learning agents are configured to be instantiated in parallel. 
     
     
       3. The method of  claim 1 , further comprising:
 storing, by the at least one processor, at least one record of the at least one instruction output by the model orchestration large language model; 
 determining, by the at least one processor, the at least one data record processing machine learning agent associated with the at least one instruction; 
 determining, by the at least one processor, the at least one response associated with the at least one data record processing machine learning agent; 
 generating, by the at least one processor, at least one explainability prompt configured to elicit a natural language explanation from the model orchestration large language model; and 
 inputting, by the at least one processor, the at least one explainability prompt, the at least one record of the at least one instruction, the at least one data record processing machine learning agent and the at least one response into the model orchestration large language model to output the natural language explanation based at least in part on trained parameters of the model orchestration large language model and the at least one context attribute; and 
 causing to display, by the at least one processor, the natural language explanation in the GUI rendered on the user computing device. 
 
     
     
       4. The method of  claim 1 , wherein the at least one data record processing machine learning agent is at least two data record processing machine learning agents;
 wherein at least one first data record processing machine learning agent is adversarial to at least one second data record processing machine learning agent. 
 
     
     
       5. The method of  claim 4 , wherein the at least one first data record processing machine learning agent is configured to output the at least one response; and
 wherein the at least one second data record processing machine learning agent is configured to:
 ingest the at least one response from the at least one first data record processing machine learning agent; 
 determine a correctness assessment based at least in part on correctness assessment machine learning parameters; and 
 refine the at least one response based at least in part on the correctness assessment. 
 
 
     
     
       6. The method of  claim 1 , further comprising:
 inputting, by the at least one processor, at least one compliance rule into at least one compliance verification machine learning agent to output at least one compliance verification prompt based at least in part on a plurality of compliance verification parameters;
 wherein the at least one compliance verification prompt is configured to cause the model orchestration large language model to verify compliance of the at least one natural language response with the at least one compliance rule; and 
 
 inputting, by the at least one processor, the at least one compliance verification prompt into the model orchestration large language model to output at least one compliance verification of the at least one natural language model based at least in part on the trained parameters of the model orchestration large language model and the at least one context attribute. 
 
     
     
       7. The method of  claim 6 , further comprising:
 inputting, by the at least one processor, based on the at least one compliance verification being representative of the at least one natural language response being non-compliant, the natural language response into at least one compliance machine learning agent to output a variation to the natural language response; 
 inputting, by the at least one processor, the at least one compliance verification prompt and the variation to the natural language response into the model orchestration large language model to output at least one new compliance verification of the variation to the at least one natural language model based at least in part on the trained parameters of the model orchestration large language model and the at least one context attribute. 
 
     
     
       8. The method of  claim 6 , wherein the at least one compliance verification comprises at least one of:
 a pass indicative of the at least one natural language response being compliant, or 
 a fail indicative of the at least one natural language response being non-compliant. 
 
     
     
       9. The method of  claim 1 , wherein the at least one instruction comprises at least one programmatic step comprising at least one of:
 at least one database query, 
 at least one application programming interface (API) call, or 
 at least one internet search query. 
 
     
     
       10. The method of  claim 9 , further comprising:
 tracking, by the at least one processor, the at least one instruction; 
 generating, by the at least one processor, at least one model explainability prompt representative of the at least one instruction;
 wherein the at least one model explainability prompt is configured to cause the model orchestration large language model to output a natural language explanation of at least one of:
 the at least one data record processing machine learning agent, or 
 the at least one programmatic step; and 
 
 
 inputting, by the at least one processor, the at least one model explainability prompt into the model orchestration large language model to output the natural language explanation based at least in part on the trained parameters of the model orchestration large language model and the at least one context attribute. 
 
     
     
       11. A system comprising:
 at least one processor in communication with at least one non-transitory computer readable medium having software instructions stored thereon, wherein the at least one processor, upon execution of the software instructions, is configured to:
 receive, from at least one user computing device associated with a user, a user-provided data record query comprising a natural language request to perform at least one action with at least one data record; 
 determine a user profile associated with the user;
 wherein the user profile comprises user persona attributes; 
 wherein the user persona attributes comprise at least one of:
 a user role within an organization, or 
 at least one security parameter associated with the user; 
 
 
 generate based on the user persona attributes of the user profile, at least one context query to at least one data source so as to obtain at least one context attribute representative of additional context specific to the user associated with the user-provided data record query; 
 modify a model orchestration large language model runtime of the model orchestration large language model based at least in part on the at least one context attribute so as to customize the context of the model orchestration large language model for the user persona attributes; 
 input the natural language request of the data record query as a data record query prompt into the model orchestration large language model to output at least one instruction to at least one data record processing machine learning agent of a plurality of data record processing machine learning agents based at least in part on trained parameters of the model orchestration large language model and the at least one context attribute; 
 input the at least one instruction into the at least one data record processing machine learning agent to output at least one response; 
 input the at least one response as a response prompt into the model orchestration large language model to output to the user computing device at least one natural language response representative of the at least one action based at least in part on trained parameters of the model orchestration large language model and the at least one context attribute; and 
 cause to display the at least one natural language response in a graphical user interface (GUI) rendered on the user computing device. 
 
 
     
     
       12. The system of  claim 11 , wherein the plurality of data record processing machine learning agents are configured to be instantiated in parallel. 
     
     
       13. The system of  claim 11 , wherein the at least one processor is further configured to:
 storing, by the at least one processor, at least one record of the at least one instruction output by the model orchestration large language model; 
 determining, by the at least one processor, the at least one data record processing machine learning agent associated with the at least one instruction; 
 determining, by the at least one processor, the at least one response associated with the at least one data record processing machine learning agent; 
 generating, by the at least one processor, at least one explainability prompt configured to elicit a natural language explanation from the model orchestration large language model; and 
 inputting, by the at least one processor, the at least one explainability prompt, the at least one record of the at least one instruction, the at least one data record processing machine learning agent and the at least one response into the model orchestration large language model to output the natural language explanation based at least in part on trained parameters of the model orchestration large language model and the at least one context attribute; and 
 causing to display, by the at least one processor, the natural language explanation in the GUI rendered on the user computing device. 
 
     
     
       14. The system of  claim 11 , wherein the at least one data record processing machine learning agent is at least two data record processing machine learning agents;
 wherein at least one first data record processing machine learning agent is adversarial to at least one second data record processing machine learning agent. 
 
     
     
       15. The system of  claim 14 , wherein the at least one first data record processing machine learning agent is configured to output the at least one response; and
 wherein the at least one second data record processing machine learning agent is configured to:
 ingest the at least one response from the at least one first data record processing machine learning agent; 
 determine a correctness assessment based at least in part on correctness assessment machine learning parameters; and 
 
 refine the at least one response based at least in part on the correctness assessment. 
 
     
     
       16. The system of  claim 11 , wherein the at least one processor is further configured to:
 input at least one compliance rule into at least one compliance verification machine learning agent to output at least one compliance verification prompt based at least in part on a plurality of compliance verification parameters;
 wherein the at least one compliance verification prompt is configured to cause the model orchestration large language model to verify compliance of the at least one natural language response with the at least one compliance rule; and 
 
 input the at least one compliance verification prompt into the model orchestration large language model to output at least one compliance verification of the at least one natural language model based at least in part on the trained parameters of the model orchestration large language model and the at least one context attribute. 
 
     
     
       17. The system of  claim 16 , wherein the at least one processor is further configured to:
 input based on the at least one compliance verification being representative of the at least one natural language response being non-compliant, the natural language response into at least one compliance machine learning agent to output a variation to the natural language response; 
 input the at least one compliance verification prompt and the variation to the natural language response into the model orchestration large language model to output at least one new compliance verification of the variation to the at least one natural language model based at least in part on the trained parameters of the model orchestration large language model and the at least one context attribute. 
 
     
     
       18. The system of  claim 16 , wherein the at least one compliance verification comprises at least one of:
 a pass indicative of the at least one natural language response being compliant, or 
 a fail indicative of the at least one natural language response being non-compliant. 
 
     
     
       19. The system of  claim 11 , wherein the at least one instruction comprises at least one programmatic step comprising at least one of:
 at least one database query, 
 at least one application programming interface (API) call, or 
 at least one internet search query. 
 
     
     
       20. The system of  claim 19 , wherein the at least one processor is further configured to:
 track the at least one instruction; 
 generate at least one model explainability prompt representative of the at least one instruction;
 wherein the at least one model explainability prompt is configured to cause the model orchestration large language model to output a natural language explanation of at least one of:
 the at least one data record processing machine learning agent, or 
 the at least one programmatic step; and 
 
 
 input the at least one model explainability prompt into the model orchestration large language model to output the natural language explanation based at least in part on the trained parameters of the model orchestration large language model and the at least one context attribute.","Broadridge Financial Solutions, Inc.",,,,20240813,20231006,20231006,US,US-202318482731-A,92217361,,
CN-117113339-A,一种大模型风险评估方法、装置、存储介质及电子设备,"The embodiment of the application discloses a large model risk assessment method, a large model risk assessment device, a storage medium and electronic equipment, wherein the large model risk assessment method comprises the following steps: generating model risk assessment data based on a prompt attack task data set and a risk attack instruction data set aiming at a target artificial intelligent content generation model, obtaining model content generation response data based on the model risk assessment data by adopting the target artificial intelligent content generation model, detecting risk attack content of the model content generation response data, obtaining risk content detection parameters, and determining prompt injection attack detection results aiming at the target artificial intelligent content generation model based on the risk content detection parameters.",,北京奇虎科技有限公司,ZOU QUANCHEN | ZHANG DEYUE | YANG DONGDONG | HAN DONG | XU CHANGKAI,G06F21/554 | G06F21/577 | G06F16/90344,G06F21/55 | G06F16/903 | G06F21/57,2023-11-24,20230927,20230927,CN,CN-202311262657-A,88802314,2023.0,"一种大模型风险评估方法、装置、存储介质及电子设备 the embodiment of the application discloses a large model risk assessment method, a large model risk assessment device, a storage medium and electronic equipment, wherein the large model risk assessment method comprises the following steps: generating model risk assessment data based on a prompt attack task data set and a risk attack instruction data set aiming at a target artificial intelligent content generation model, obtaining model content generation response data based on the model risk assessment data by adopting the target artificial intelligent content generation model, detecting risk attack content of the model content generation response data, obtaining risk content detection parameters, and determining prompt injection attack detection results aiming at the target artificial intelligent content generation model based on the risk content detection parameters. "
US-2024386103-A1,Signing large language model prompts to prevent unintended response,"A technique to prevent a prompt injection attack utilizes a security agent to sign a large language model prompt with a secret that is isolated from the user application or device that generates a user prompt. The secret is tailored for a specific user identifier and session identifier. The large language model is instructed to repeat the secret in each response. The security agent retrieves the response from the large language model and checks for the secret. When the secret is not part of the response, an error message is forwarded to the user application instead of the response.","What is claimed: 
     
         1 . A system comprising:
 a processor; and   a memory that stores a program configured to be executed by the processor, the program comprising instructions that when executed by the processor perform acts that:   provide instructions to a large language model (LLM), wherein the instructions indicate that the LLM is to repeat a secret in a response generated by the LLM when given a user prompt that includes the secret, wherein the instructions indicate a scope of the response that defines data to be included and not included in the response;   receive a network message from a user application, wherein the network message comprises the user prompt destined for the LLM, a user identifier and a session identifier;   obtain a secret associated with the user identifier and the session identifier;   generate a LLM prompt comprising the secret and the user prompt;   receive a response from the LLM based on the LLM prompt; and   when the response fails to include the secret, return an error message.   
     
     
         2 . The system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 when the response from the large language model includes the secret, extract the secret from the response and return a remaining portion of the response to the user application.   
     
     
         3 . The system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 store the secret in a memory not accessible by the large language model and the user application.   
     
     
         4 . The system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 associate a turn count and a turn count limit for the secret; and   when the turn count of the secret exceeds a limit of the turn count, create a new secret for the user identifier and session identifier.   
     
     
         5 . The system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 associate a turn count for the secret; and   update the turn count for each LLM prompt generated.   
     
     
         6 . The system of  claim 1 , wherein the secret comprises an ordered sequence of natural language words. 
     
     
         7 . The system of  claim 1 , wherein the session identifier is unique for each user session. 
     
     
         8 . The system of  claim 1 , wherein the user identifier is an Internet Protocol address associated with a user computing device. 
     
     
         9 . A computer-implemented method, comprising:
 providing instructions to a large language model (LLM) for answering a conversational query, wherein the instructions indicate a scope of a response to the conversational query, wherein the conversational query is associated with a single user session;   receiving a first user prompt of the conversational query for the LLM to return a first response, wherein the first user prompt is associated with a user identifier and a session identifier of the single user session;   constructing a secret for the user identifier and the session identifier;   creating a LLM prompt comprising the secret and the first user prompt;   receiving the first response from the large language model given the LLM prompt;   detecting a prompt injection attack when the first response fails to include the secret in the first response; and   returning an error message upon detection of the prompt injection attack.   
     
     
         10 . The computer-implemented method of  claim 9 , comprising:
 receiving a second user prompt of the conversational query; and   creating a second LLM prompt comprises the secret and the second user prompt.   
     
     
         11 . The computer-implemented method of  claim 9 , comprising:
 associating a turn count with the secret; and   creating a new secret when the turn count exceeds a turn count limit.   
     
     
         12 . The computer-implemented method of  claim 9 , comprising:
 associating a turn count with the secret; and   updating the turn count when the secret is used in each LLM prompt.   
     
     
         13 . The computer-implemented method of  claim 9 , comprising:
 extracting the secret from the response and returning a remaining portion of the response.   
     
     
         14 . The computer-implemented method of  claim 9 , comprising:
 receiving the user prompt via a network from a user application; and   storing the secret in a memory not accessible by the large language model and the user application.   
     
     
         15 . The computer-implemented method of  claim 9 , wherein the large language model is a neural transformer model with attention. 
     
     
         16 . A hardware device having stored thereon computer executable instructions that are structured to be executable by a processor of a computing device to thereby cause the computing device to perform actions that:
 provide a system prompt to a large language model (LLM) that comprises instructions that instruct the LLM to repeat a secret in a response generated by the large language model (LLM) given a user prompt and a goal for the response generated by the LLM, wherein the goal indicates a restriction on actions not to be performed by the LLM;   receive a first network message from a user application, wherein the first network message comprises a first user prompt destined for the LLM, a user identifier and a session identifier;   obtain a first secret associated with the user identifier and the session identifier;   generate a LLM prompt comprising the first secret and the first user prompt;   receive a response from the LLM based on the LLM prompt; and   when the response fails to include the first secret, return an error message to the user application.   
     
     
         17 . The hardware device of  claim 16 , having stored thereon computer executable instructions that are structured to be executable by the processor of the computing device to thereby cause the computing device to perform actions that:
 generate a new secret when usage of the first secret exceeds a threshold limit associated with the first secret.   
     
     
         18 . The hardware device of  claim 16 , having stored thereon computer executable instructions that are structured to be executable by the processor of the computing device to thereby cause the computing device to perform actions that:
 store the first secret in a memory not accessible by the large language model and the user application.   
     
     
         19 . The hardware device of  claim 16 , having stored thereon computer executable instructions that are structured to be executable by the processor of the computing device to thereby cause the computing device to perform actions that:
 receive a second network message from the user application, wherein second network message comprises a second user prompt destined for the LLM, wherein the second network message is associated with the user identifier and the session identifier,   obtain the first secret of the user identifier and the session identifier; and   generate a second LLM prompt comprising the first secret and the second user prompt.   
     
     
         20 . The hardware device of  claim 16 , wherein the LLM comprises a neural transformer model with attention.","Microsoft Technology Licensing, Llc.","CLEMENT, Colin Bruce | FU, SHENGYU | SUNDARESAN, NEELAKANTAN | YOU, Dongjiang",G06F2221/034 | G06F21/56 | G06F2221/034 | G06F21/56 | G06F2221/034 | G06F21/56,G06F21/56,2024-11-21,20230819,20230517,US,US-202318235836-A,93464563,2024.0,"signing large language model prompts to prevent unintended response a technique to prevent a prompt injection attack utilizes a security agent to sign a large language model prompt with a secret that is isolated from the user application or device that generates a user prompt. the secret is tailored for a specific user identifier and session identifier. the large language model is instructed to repeat the secret in each response. the security agent retrieves the response from the large language model and checks for the secret. when the secret is not part of the response, an error message is forwarded to the user application instead of the response. what is claimed: 
     
         1 . a system comprising:
 a processor; and   a memory that stores a program configured to be executed by the processor, the program comprising instructions that when executed by the processor perform acts that:   provide instructions to a large language model (llm), wherein the instructions indicate that the llm is to repeat a secret in a response generated by the llm when given a user prompt that includes the secret, wherein the instructions indicate a scope of the response that defines data to be included and not included in the response;   receive a network message from a user application, wherein the network message comprises the user prompt destined for the llm, a user identifier and a session identifier;   obtain a secret associated with the user identifier and the session identifier;   generate a llm prompt comprising the secret and the user prompt;   receive a response from the llm based on the llm prompt; and   when the response fails to include the secret, return an error message.   
     
     
         2 . the system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 when the response from the large language model includes the secret, extract the secret from the response and return a remaining portion of the response to the user application.   
     
     
         3 . the system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 store the secret in a memory not accessible by the large language model and the user application.   
     
     
         4 . the system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 associate a turn count and a turn count limit for the secret; and   when the turn count of the secret exceeds a limit of the turn count, create a new secret for the user identifier and session identifier.   
     
     
         5 . the system of  claim 1 , wherein the program comprises instructions that when executed by the processor perform acts that:
 associate a turn count for the secret; and   update the turn count for each llm prompt generated.   
     
     
         6 . the system of  claim 1 , wherein the secret comprises an ordered sequence of natural language words. 
     
     
         7 . the system of  claim 1 , wherein the session identifier is unique for each user session. 
     
     
         8 . the system of  claim 1 , wherein the user identifier is an internet protocol address associated with a user computing device. 
     
     
         9 . a computer-implemented method, comprising:
 providing instructions to a large language model (llm) for answering a conversational query, wherein the instructions indicate a scope of a response to the conversational query, wherein the conversational query is associated with a single user session;   receiving a first user prompt of the conversational query for the llm to return a first response, wherein the first user prompt is associated with a user identifier and a session identifier of the single user session;   constructing a secret for the user identifier and the session identifier;   creating a llm prompt comprising the secret and the first user prompt;   receiving the first response from the large language model given the llm prompt;   detecting a prompt injection attack when the first response fails to include the secret in the first response; and   returning an error message upon detection of the prompt injection attack.   
     
     
         10 . the computer-implemented method of  claim 9 , comprising:
 receiving a second user prompt of the conversational query; and   creating a second llm prompt comprises the secret and the second user prompt.   
     
     
         11 . the computer-implemented method of  claim 9 , comprising:
 associating a turn count with the secret; and   creating a new secret when the turn count exceeds a turn count limit.   
     
     
         12 . the computer-implemented method of  claim 9 , comprising:
 associating a turn count with the secret; and   updating the turn count when the secret is used in each llm prompt.   
     
     
         13 . the computer-implemented method of  claim 9 , comprising:
 extracting the secret from the response and returning a remaining portion of the response.   
     
     
         14 . the computer-implemented method of  claim 9 , comprising:
 receiving the user prompt via a network from a user application; and   storing the secret in a memory not accessible by the large language model and the user application.   
     
     
         15 . the computer-implemented method of  claim 9 , wherein the large language model is a neural transformer model with attention. 
     
     
         16 . a hardware device having stored thereon computer executable instructions that are structured to be executable by a processor of a computing device to thereby cause the computing device to perform actions that:
 provide a system prompt to a large language model (llm) that comprises instructions that instruct the llm to repeat a secret in a response generated by the large language model (llm) given a user prompt and a goal for the response generated by the llm, wherein the goal indicates a restriction on actions not to be performed by the llm;   receive a first network message from a user application, wherein the first network message comprises a first user prompt destined for the llm, a user identifier and a session identifier;   obtain a first secret associated with the user identifier and the session identifier;   generate a llm prompt comprising the first secret and the first user prompt;   receive a response from the llm based on the llm prompt; and   when the response fails to include the first secret, return an error message to the user application.   
     
     
         17 . the hardware device of  claim 16 , having stored thereon computer executable instructions that are structured to be executable by the processor of the computing device to thereby cause the computing device to perform actions that:
 generate a new secret when usage of the first secret exceeds a threshold limit associated with the first secret.   
     
     
         18 . the hardware device of  claim 16 , having stored thereon computer executable instructions that are structured to be executable by the processor of the computing device to thereby cause the computing device to perform actions that:
 store the first secret in a memory not accessible by the large language model and the user application.   
     
     
         19 . the hardware device of  claim 16 , having stored thereon computer executable instructions that are structured to be executable by the processor of the computing device to thereby cause the computing device to perform actions that:
 receive a second network message from the user application, wherein second network message comprises a second user prompt destined for the llm, wherein the second network message is associated with the user identifier and the session identifier,   obtain the first secret of the user identifier and the session identifier; and   generate a second llm prompt comprising the first secret and the second user prompt.   
     
     
         20 . the hardware device of  claim 16 , wherein the llm comprises a neural transformer model with attention."
US-2024403560-A1,Prevention of prompt injection attacks on large language models by tokenization of structured data elements,Systems and methods for implementing prevention of prompt injection attacks on large language models by tokenization of structured data elements is presented. The systems and methods replace one or more data elements in a database response with one or more tokens to produce a tokenized database response. The systems and methods provide the tokenized database response to a large language model (LLM). The systems and methods receive a tokenized LLM output that includes at least one of the one or more tokens. The systems and methods produce a detokenized LLM output by replacing the one or more tokens in the tokenized LLM output with the one or more data elements.,"What is claimed is: 
     
         1 . A method comprising:
 replacing one or more data elements in a database response with one or more tokens to produce a tokenized database response;   providing the tokenized database response to a large language model (LLM);   receiving a tokenized LLM output that comprises at least one of the one or more tokens; and   producing, by a processing device, a detokenized LLM output by replacing the at least one of the one or more tokens in the tokenized LLM output with at least one of the one or more data elements.   
     
     
         2 . The method of  claim 1 , wherein the one or more data elements are a plurality of data elements, and wherein the database response corresponds to a query of a database, the method further comprising:
 identifying a first portion of the plurality of data elements that are modifiable by an external entity; and   replacing the first portion of the plurality of data elements with the one or more tokens.   
     
     
         3 . The method of  claim 2 , further comprising:
 identifying a second portion of the plurality of data elements that are not modifiable by the external entity; and   maintaining the second portion of the plurality of data elements in the tokenized database response.   
     
     
         4 . The method of  claim 2 , further comprising:
 extracting metadata from the database response; and   analyzing the metadata to determine which of the plurality of data elements are modifiable by the external entity.   
     
     
         5 . The method of  claim 1 , further comprising:
 evaluating the one or more data elements in the database response to generate heuristics data corresponding to the one or more data elements;   evaluating the heuristics data to determine which of the one or more data elements comprise malicious content; and   replacing the one or more data elements comprising the malicious content with the one or more tokens.   
     
     
         6 . The method of  claim 1 , further comprising:
 encoding one or more subsequent data elements in a subsequent database response to produce an encoded database response comprising one or more encoded data elements;   providing the encoded database response to the LLM, wherein the LLM produces an encoded LLM output based on the encoded database response, wherein the encoded LLM output comprises at least one of the one or more encoded data elements; and   decoding the at least one of the one or more encoded data elements in the encoded LLM output to produce a decoded LLM output.   
     
     
         7 . The method of  claim 1 , wherein the tokenized LLM output comprises a tokenized API call, the method further comprising:
 producing a detokenized API call by replacing the at least one of the one or more tokens in the tokenized API call with at least one of the one or more data elements;   executing the detokenized API call to produce the database response; and   providing the detokenized LLM output to a user interface.   
     
     
         8 . A system comprising:
 a processing device; and   a memory to store instructions that, when executed by the processing device cause the processing device to:
 replace one or more data elements in a database response with one or more tokens to produce a tokenized database response; 
 provide the tokenized database response to a large language model (LLM); 
 receive a tokenized LLM output that comprises at least one of the one or more tokens; and 
 produce a detokenized LLM output by replacing the at least one of the one or more tokens in the tokenized LLM output with at least one of the one or more data elements. 
   
     
     
         9 . The system of  claim 8 , wherein the one or more data elements are a plurality of data elements, the database response corresponds to a query of a database, and wherein the processing device, responsive to executing the instructions, further causes the system to:
 identify a first portion of the plurality of data elements that are modifiable by an external entity;   identifying a second portion of the plurality of data elements that are not modifiable by the external entity;   replace the first portion of the plurality of data elements with the one or more tokens; and   maintain the second portion of the plurality of data elements in the tokenized database response.   
     
     
         10 . The system of  claim 9 , wherein the processing device, responsive to executing the instructions, further causes the system to:
 extract metadata from the database response; and   analyze the metadata to determine which of the plurality of data elements are modifiable by the external entity.   
     
     
         11 . The system of  claim 8 , wherein the processing device, responsive to executing the instructions, further causes the system to:
 evaluate the one or more data elements in the database response to generate heuristics data corresponding to the one or more data elements;   evaluate the heuristics data to determine which of the one or more data elements comprise malicious content; and   replace the one or more data elements comprising the malicious content with the one or more tokens.   
     
     
         12 . The system of  claim 8 , wherein the processing device, responsive to executing the instructions, further causes the system to:
 encode one or more subsequent data elements in a subsequent database response to produce an encoded database response comprising one or more encoded data elements;   provide the encoded database response to the LLM, wherein the LLM produces an encoded LLM output based on the encoded database response, wherein the encoded LLM output comprises at least one of the one or more encoded data elements; and   decode the at least one of the one or more encoded data elements in the encoded LLM output to produce a decoded LLM output.   
     
     
         13 . The system of  claim 8 , wherein the LLM is a first LLM, and wherein the processing device, responsive to executing the instructions, further causes the system to:
 provide the tokenized LLM output and an instruction to a second LLM; and   perform, by the second LLM, the instruction based on the tokenized LLM output.   
     
     
         14 . The system of  claim 8 , wherein the tokenized LLM output comprises a tokenized API call, and wherein the processing device, responsive to executing the instructions, further causes the system to:
 produce a detokenized API call by replacing the at least one of the one or more tokens in the tokenized API call with at least one of the one or more data elements;   execute the detokenized API call to produce the database response; and   provide the detokenized LLM output to a user interface.   
     
     
         15 . A non-transitory computer readable medium, having instructions stored thereon which, when executed by a processing device, cause the processing device to:
 replace one or more data elements in a database response with one or more tokens to produce a tokenized database response;   provide the tokenized database response to a large language model (LLM);   receive a tokenized LLM output that comprises at least one of the one or more tokens; and   produce, by the processing device, a detokenized LLM output by replacing the at least one of the one or more tokens in the tokenized LLM output with at least one of the one or more data elements.   
     
     
         16 . The non-transitory computer readable medium of  claim 15 , wherein the one or more data elements are a plurality of data elements, the database response corresponds to a query of a database, and wherein the processing device is to:
 identify a first portion of the plurality of data elements that are modifiable by an external entity;   identify a second portion of the plurality of data elements that are not modifiable by the external entity;   replace the first portion of the plurality of data elements with the one or more tokens; and   maintain the second portion of the plurality of data elements in the tokenized database response.   
     
     
         17 . The non-transitory computer readable medium of  claim 16 , wherein the processing device is to:
 extract metadata from the database response; and   analyze the metadata to determine which of the plurality of data elements are modifiable by the external entity.   
     
     
         18 . The non-transitory computer readable medium of  claim 15 , wherein the processing device is to:
 evaluate the one or more data elements in the database response to generate heuristics data corresponding to the one or more data elements;   evaluate the heuristics data to determine which of the one or more data elements comprise malicious content; and   replace the one or more data elements comprising the malicious content with the one or more tokens.   
     
     
         19 . The non-transitory computer readable medium of  claim 15 , wherein the processing device is to:
 encode one or more subsequent data elements in a subsequent database response to produce an encoded database response comprising one or more encoded data elements;   provide the encoded database response to the LLM, wherein the LLM produces an encoded LLM output based on the encoded database response, wherein the encoded LLM output comprises at least one of the one or more encoded data elements; and   decode the at least one of the one or more encoded data elements in the encoded LLM output to produce a decoded LLM output.   
     
     
         20 . The non-transitory computer readable medium of  claim 15 , wherein the tokenized LLM output comprises a tokenized API call, and wherein the processing device is to:
 produce a detokenized API call by replacing the at least one of the one or more tokens in the tokenized API call with at least one of the one or more data elements;   execute the detokenized API call to produce the database response; and   provide the detokenized LLM output to a user interface.","Crowdstrike, Inc.","Radu, Daniel | RADU, MARIAN | KRASSER, SVEN",G06F16/908 | G06F40/284 | G06F21/6245 | G06F21/604 | G06F21/6254 | G06F40/284 | G06F21/6227 | G06F16/908 | G06F40/284,G06F16/908 | G06F40/284,2024-12-05,20230808,20230602,US,US-202318446314-A,91334899,2024.0,"prevention of prompt injection attacks on large language models by tokenization of structured data elements systems and methods for implementing prevention of prompt injection attacks on large language models by tokenization of structured data elements is presented. the systems and methods replace one or more data elements in a database response with one or more tokens to produce a tokenized database response. the systems and methods provide the tokenized database response to a large language model (llm). the systems and methods receive a tokenized llm output that includes at least one of the one or more tokens. the systems and methods produce a detokenized llm output by replacing the one or more tokens in the tokenized llm output with the one or more data elements. what is claimed is: 
     
         1 . a method comprising:
 replacing one or more data elements in a database response with one or more tokens to produce a tokenized database response;   providing the tokenized database response to a large language model (llm);   receiving a tokenized llm output that comprises at least one of the one or more tokens; and   producing, by a processing device, a detokenized llm output by replacing the at least one of the one or more tokens in the tokenized llm output with at least one of the one or more data elements.   
     
     
         2 . the method of  claim 1 , wherein the one or more data elements are a plurality of data elements, and wherein the database response corresponds to a query of a database, the method further comprising:
 identifying a first portion of the plurality of data elements that are modifiable by an external entity; and   replacing the first portion of the plurality of data elements with the one or more tokens.   
     
     
         3 . the method of  claim 2 , further comprising:
 identifying a second portion of the plurality of data elements that are not modifiable by the external entity; and   maintaining the second portion of the plurality of data elements in the tokenized database response.   
     
     
         4 . the method of  claim 2 , further comprising:
 extracting metadata from the database response; and   analyzing the metadata to determine which of the plurality of data elements are modifiable by the external entity.   
     
     
         5 . the method of  claim 1 , further comprising:
 evaluating the one or more data elements in the database response to generate heuristics data corresponding to the one or more data elements;   evaluating the heuristics data to determine which of the one or more data elements comprise malicious content; and   replacing the one or more data elements comprising the malicious content with the one or more tokens.   
     
     
         6 . the method of  claim 1 , further comprising:
 encoding one or more subsequent data elements in a subsequent database response to produce an encoded database response comprising one or more encoded data elements;   providing the encoded database response to the llm, wherein the llm produces an encoded llm output based on the encoded database response, wherein the encoded llm output comprises at least one of the one or more encoded data elements; and   decoding the at least one of the one or more encoded data elements in the encoded llm output to produce a decoded llm output.   
     
     
         7 . the method of  claim 1 , wherein the tokenized llm output comprises a tokenized api call, the method further comprising:
 producing a detokenized api call by replacing the at least one of the one or more tokens in the tokenized api call with at least one of the one or more data elements;   executing the detokenized api call to produce the database response; and   providing the detokenized llm output to a user interface.   
     
     
         8 . a system comprising:
 a processing device; and   a memory to store instructions that, when executed by the processing device cause the processing device to:
 replace one or more data elements in a database response with one or more tokens to produce a tokenized database response; 
 provide the tokenized database response to a large language model (llm); 
 receive a tokenized llm output that comprises at least one of the one or more tokens; and 
 produce a detokenized llm output by replacing the at least one of the one or more tokens in the tokenized llm output with at least one of the one or more data elements. 
   
     
     
         9 . the system of  claim 8 , wherein the one or more data elements are a plurality of data elements, the database response corresponds to a query of a database, and wherein the processing device, responsive to executing the instructions, further causes the system to:
 identify a first portion of the plurality of data elements that are modifiable by an external entity;   identifying a second portion of the plurality of data elements that are not modifiable by the external entity;   replace the first portion of the plurality of data elements with the one or more tokens; and   maintain the second portion of the plurality of data elements in the tokenized database response.   
     
     
         10 . the system of  claim 9 , wherein the processing device, responsive to executing the instructions, further causes the system to:
 extract metadata from the database response; and   analyze the metadata to determine which of the plurality of data elements are modifiable by the external entity.   
     
     
         11 . the system of  claim 8 , wherein the processing device, responsive to executing the instructions, further causes the system to:
 evaluate the one or more data elements in the database response to generate heuristics data corresponding to the one or more data elements;   evaluate the heuristics data to determine which of the one or more data elements comprise malicious content; and   replace the one or more data elements comprising the malicious content with the one or more tokens.   
     
     
         12 . the system of  claim 8 , wherein the processing device, responsive to executing the instructions, further causes the system to:
 encode one or more subsequent data elements in a subsequent database response to produce an encoded database response comprising one or more encoded data elements;   provide the encoded database response to the llm, wherein the llm produces an encoded llm output based on the encoded database response, wherein the encoded llm output comprises at least one of the one or more encoded data elements; and   decode the at least one of the one or more encoded data elements in the encoded llm output to produce a decoded llm output.   
     
     
         13 . the system of  claim 8 , wherein the llm is a first llm, and wherein the processing device, responsive to executing the instructions, further causes the system to:
 provide the tokenized llm output and an instruction to a second llm; and   perform, by the second llm, the instruction based on the tokenized llm output.   
     
     
         14 . the system of  claim 8 , wherein the tokenized llm output comprises a tokenized api call, and wherein the processing device, responsive to executing the instructions, further causes the system to:
 produce a detokenized api call by replacing the at least one of the one or more tokens in the tokenized api call with at least one of the one or more data elements;   execute the detokenized api call to produce the database response; and   provide the detokenized llm output to a user interface.   
     
     
         15 . a non-transitory computer readable medium, having instructions stored thereon which, when executed by a processing device, cause the processing device to:
 replace one or more data elements in a database response with one or more tokens to produce a tokenized database response;   provide the tokenized database response to a large language model (llm);   receive a tokenized llm output that comprises at least one of the one or more tokens; and   produce, by the processing device, a detokenized llm output by replacing the at least one of the one or more tokens in the tokenized llm output with at least one of the one or more data elements.   
     
     
         16 . the non-transitory computer readable medium of  claim 15 , wherein the one or more data elements are a plurality of data elements, the database response corresponds to a query of a database, and wherein the processing device is to:
 identify a first portion of the plurality of data elements that are modifiable by an external entity;   identify a second portion of the plurality of data elements that are not modifiable by the external entity;   replace the first portion of the plurality of data elements with the one or more tokens; and   maintain the second portion of the plurality of data elements in the tokenized database response.   
     
     
         17 . the non-transitory computer readable medium of  claim 16 , wherein the processing device is to:
 extract metadata from the database response; and   analyze the metadata to determine which of the plurality of data elements are modifiable by the external entity.   
     
     
         18 . the non-transitory computer readable medium of  claim 15 , wherein the processing device is to:
 evaluate the one or more data elements in the database response to generate heuristics data corresponding to the one or more data elements;   evaluate the heuristics data to determine which of the one or more data elements comprise malicious content; and   replace the one or more data elements comprising the malicious content with the one or more tokens.   
     
     
         19 . the non-transitory computer readable medium of  claim 15 , wherein the processing device is to:
 encode one or more subsequent data elements in a subsequent database response to produce an encoded database response comprising one or more encoded data elements;   provide the encoded database response to the llm, wherein the llm produces an encoded llm output based on the encoded database response, wherein the encoded llm output comprises at least one of the one or more encoded data elements; and   decode the at least one of the one or more encoded data elements in the encoded llm output to produce a decoded llm output.   
     
     
         20 . the non-transitory computer readable medium of  claim 15 , wherein the tokenized llm output comprises a tokenized api call, and wherein the processing device is to:
 produce a detokenized api call by replacing the at least one of the one or more tokens in the tokenized api call with at least one of the one or more data elements;   execute the detokenized api call to produce the database response; and   provide the detokenized llm output to a user interface."
US-11916767-B1,Security analysis agents,Embodiments are directed to security analysis agents. Events associated with a computing environment may be provided. Prompt fragments may be determined based on the events. A prompt may be generated for a large language model (LLM) based on a prompt template and the prompt fragments such that the prompt fragments may be included in the prompt and provided to the LLM. Actions for evaluating the events may be determined based on the LLM response. These actions may be executed to evaluate the events. Portions of the response that correspond to the prompt fragments may be determined. A performance score may be determined for each prompt fragment based on its corresponding portion of the response such that the prompt may be modified to exclude a portion of the prompt fragments that have a performance score less than a threshold value.,"What is claimed as new and desired to be protected by Letters Patent of the United States is: 
     
       1. A method for monitoring activity in a computing environment using one or more processors to execute instructions that are configured to cause actions, comprising:
 providing one or more events associated with one or more resources or one or more activities in the computing environment; 
 determining one or more prompt fragments based on the one or more events; 
 generating a data structure for a prompt for training a large language model (LLM) based on a prompt template and the one or more prompt fragments, wherein the one or more prompt fragments are included in the prompt, and wherein information associated with the one or more events is included in the prompt; and 
 employing the prompt to train the LLM by performing further actions, including:
 determining one or more actions for evaluating the one or more events based on machine-readable information included in a response provided by the LLM being trained; 
 executing the one or more actions to evaluate the one or more events, wherein a portion of the one or more events are classified based on the evaluation; 
 determining one or more portions of the response based on the one or more prompt fragments, wherein each determined portion of the response corresponds to at least one of the one or more prompt fragments; and 
 determining a performance score for each prompt fragment based on its corresponding determined portion of the response by causing further actions, including:
 generating one or more synthetic events based on one or more scenarios, wherein an expected classification of each synthetic event is known in advance; 
 modifying the prompt to include the one or more synthetic events, wherein the prompt is modified to exclude each portion of the prompt fragments associated with a value of the performance score that is less than a threshold value, and wherein the modified prompt is provided to the LLM to generate another response; 
 determining one or more other portions of the other response based on the one or more prompt fragments, wherein each determined other portion of the other response corresponds to a prompt fragment; 
 comparing each determined other portion of the response to the expected classification of each synthetic event; and 
 updating the performance score for each prompt fragment based on the comparison; and 
 
 
 employing the modified prompt to retrain the LLM to execute one or more other actions and classify one or more other events. 
 
     
     
       2. The method of  claim 1 , wherein classifying the one or more events, further comprises:
 classifying a portion of the one or more events as one or more false positive events based on the evaluation. 
 
     
     
       3. The method of  claim 1 , further comprising:
 generating an initial prompt for the LLM based on an initial prompt template and the one or more events; and 
 in response to providing the initial prompt to the LLM, determining the prompt template and the one or more prompt fragments based on other information included in another response provided by the LLM. 
 
     
     
       4. The method of  claim 1 , further comprising:
 determining one or more metrics for each prompt fragment based on its corresponding determined portion of the response; and 
 determining the performance score for each prompt fragment based on the one or more metrics, wherein the one or more metrics include one or more of latency, correctness, resource consumption, cost, event type applicability, or redundancy. 
 
     
     
       5. The method of  claim 1 , wherein executing the one or more actions for evaluating the one or more events, further comprises:
 determining one or more evaluation tools based on the information included in the response from the LLM; 
 determining one or more parameters for the one or more evaluation tools based on the information included in the response from the LLM, wherein the one or more parameters include one or more characteristics of the one or more events; and 
 executing the one or more evaluation tools using the one or more parameters. 
 
     
     
       6. The method of  claim 1 , wherein determining the one or more actions for evaluating the one or more events, further comprises:
 determining one or more agents based on the response, wherein each agent is associated with one or more LLMs and each agent includes an agent prompt that includes one or more agent prompt fragments; and 
 employing the one or more agents to execute the one or more actions, wherein the one or more agents generate one or more action responses based on the one or more of the one or more events or the response. 
 
     
     
       7. The method of  claim 1 , wherein determining the one or more prompt fragments, further comprises:
 providing a data store that includes a plurality of prompt fragments; and 
 selecting the one or more prompt fragments from data store based on the performance score of each prompt fragment, wherein the performance score is based on one or more previous event evaluations. 
 
     
     
       8. A network computer for monitoring activity in a computing environment, comprising:
 a memory that stores at least instructions; and 
 one or more processors that execute instructions that are configured to cause actions, including:
 providing one or more events associated with one or more resources or one or more activities in the computing environment; 
 determining one or more prompt fragments based on the one or more events; 
 generating a data structure for a prompt for training a large language model (LLM) based on a prompt template and the one or more prompt fragments, wherein the one or more prompt fragments are included in the prompt, and wherein information associated with the one or more events is included in the prompt; and 
 employing the prompt to train the LLM by performing further actions, including:
 determining one or more actions for evaluating the one or more events based on machine-readable information included in a response provided by the LLM being trained; 
 executing the one or more actions to evaluate the one or more events, wherein a portion of the one or more events are classified based on the evaluation; 
 determining one or more portions of the response based on the one or more prompt fragments, wherein each determined portion of the response corresponds to at least one of the one or more prompt fragments; and 
 determining a performance score for each prompt fragment based on its corresponding determined portion of the response by performing further actions, including:
 generating one or more synthetic events based on one or more scenarios, wherein an expected classification of each synthetic event is known in advance; 
 modifying the prompt to include the one or more synthetic events, wherein the prompt is modified to exclude each portion of the prompt fragments associated with a value of the performance score that is less than a threshold value, and wherein the modified prompt is provided to the LLM to generate another response; 
 determining one or more other portions of the other response based on the one or more prompt fragments, wherein each determined other portion of the other response corresponds to a prompt fragment; 
 comparing each determined other portion of the response to the expected classification of each synthetic event; and 
 updating the performance score for each prompt fragment based on the comparison; and 
 
 
 employing the modified prompt to retrain the LLM to execute one or more other actions and classify one or more other events. 
 
 
     
     
       9. The network computer of  claim 8 , wherein classifying the one or more events, further comprises:
 classifying a portion of the one or more events as one or more false positive events based on the evaluation. 
 
     
     
       10. The network computer of  claim 8 , wherein the one or more processors execute instructions that are configured to cause actions, further comprising:
 generating an initial prompt for the LLM based on an initial prompt template and the one or more events; and 
 in response to providing the initial prompt to the LLM, determining the prompt template and the one or more prompt fragments based on other information included in another response provided by the LLM. 
 
     
     
       11. The network computer of  claim 8 , wherein the one or more processors execute instructions that are configured to cause actions, further comprising:
 determining one or more metrics for each prompt fragment based on its corresponding determined portion of the response; and 
 determining the performance score for each prompt fragment based on the one or more metrics, wherein the one or more metrics include one or more of latency, correctness, resource consumption, cost, event type applicability, or redundancy. 
 
     
     
       12. The network computer of  claim 8 , wherein executing the one or more actions for evaluating the one or more events, further comprises:
 determining one or more evaluation tools based on the information included in the response from the LLM; 
 determining one or more parameters for the one or more evaluation tools based on the information included in the response from the LLM, wherein the one or more parameters include one or more characteristics of the one or more events; and 
 executing the one or more evaluation tools using the one or more parameters. 
 
     
     
       13. The network computer of  claim 8 , wherein determining the one or more actions for evaluating the one or more events, further comprises:
 determining one or more agents based on the response, wherein each agent is associated with one or more LLMs and each agent includes an agent prompt that includes one or more agent prompt fragments; and 
 employing the one or more agents to execute the one or more actions, wherein the one or more agents generate one or more action responses based on the one or more of the one or more events or the response. 
 
     
     
       14. The network computer of  claim 8 , wherein determining the one or more prompt fragments, further comprises:
 providing a data store that includes a plurality of prompt fragments; and 
 selecting the one or more prompt fragments from data store based on the performance score of each prompt fragment, wherein the performance score is based on one or more previous event evaluations. 
 
     
     
       15. A processor readable non-transitory storage media that includes instructions configured for monitoring activity in a computing environment, wherein execution of the instructions by one or more processors on one or more network computers performs actions, comprising:
 providing one or more events associated with one or more resources or one or more activities in the computing environment; 
 determining one or more prompt fragments based on the one or more events; 
 generating a data structure for a prompt for training a large language model (LLM) based on a prompt template and the one or more prompt fragments, wherein the one or more prompt fragments are included in the prompt, and wherein information associated with the one or more events is included in the prompt; and 
 employing the prompt to train the LLM by performing further actions, including:
 determining one or more actions for evaluating the one or more events based on machine-readable information included in a response provided by the LLM being trained; 
 executing the one or more actions to evaluate the one or more events, wherein a portion of the one or more events are classified based on the evaluation; 
 determining one or more portions of the response based on the one or more prompt fragments, wherein each determined portion of the response corresponds to at least one of the one or more prompt fragments; and 
 determining a performance score for each prompt fragment based on its corresponding determined portion of the response by causing further actions, including:
 generating one or more synthetic events based on one or more scenarios, wherein an expected classification of each synthetic event is known in advance: 
 modifying the prompt to include the one or more synthetic events, wherein the prompt is modified to exclude each portion of the prompt fragments associated with a value of the performance score that is less than a threshold value, and wherein the modified prompt is provided to the LLM to generate another response; 
 determining one or more other portions of the other response based on the one or more prompt fragments, wherein each determined other portion of the other response corresponds to a prompt fragment; 
 comparing each determined other portion of the response to the expected classification of each synthetic event; and 
 updating the performance score for each prompt fragment based on the comparison; and 
 
 
 employing the modified prompt to retrain the LLM to execute one or more other actions and classify one or more other events. 
 
     
     
       16. The media of  claim 15 , wherein classifying the one or more events, further comprises:
 classifying a portion of the one or more events as one or more false positive events based on the evaluation. 
 
     
     
       17. The media of  claim 15 , further comprising:
 generating an initial prompt for the LLM based on an initial prompt template and the one or more events; and 
 in response to providing the initial prompt to the LLM, determining the prompt template and the one or more prompt fragments based on other information included in another response provided by the LLM. 
 
     
     
       18. The media of  claim 15 , further comprising:
 determining one or more metrics for each prompt fragment based on its corresponding determined portion of the response; and 
 determining the performance score for each prompt fragment based on the one or more metrics, wherein the one or more metrics include one or more of latency, correctness, resource consumption, cost, event type applicability, or redundancy. 
 
     
     
       19. The media of  claim 15 , wherein executing the one or more actions for evaluating the one or more events, further comprises:
 determining one or more evaluation tools based on the information included in the response from the LLM; 
 determining one or more parameters for the one or more evaluation tools based on the information included in the response from the LLM, wherein the one or more parameters include one or more characteristics of the one or more events; and 
 executing the one or more evaluation tools using the one or more parameters. 
 
     
     
       20. The media of  claim 15 , wherein determining the one or more prompt fragments, further comprises:
 providing a data store that includes a plurality of prompt fragments; and 
 selecting the one or more prompt fragments from data store based on the performance score of each prompt fragment, wherein the performance score is based on one or more previous event evaluations. 
 
     
     
       21. A system for monitoring activity in a computing environment:
 one or more network computers, comprising:
 a memory that stores at least instructions; and 
 one or more processors that execute instructions that are configured to cause actions, including:
 determining one or more events associated with one or more resources or one or more activities in the computing environment; 
 determining one or more prompt fragments based on the one or more events; 
 generating a data structure for a prompt for training a large language model (LLM) based on a prompt template and the one or more prompt fragments, wherein the one or more prompt fragments are included in the prompt, and wherein information associated with the one or more events is included in the prompt; and 
 employing the prompt to train the LLM by performing further actions, including:
 determining one or more actions for evaluating the one or more events based on machine-readable information included in a response provided by the LLM being trained; 
 executing the one or more actions to evaluate the one or more events, wherein a portion of the one or more events are classified based on the evaluation; 
 determining one or more portions of the response based on the one or more prompt fragments, wherein each determined portion of the response corresponds to at least one of the one or more prompt fragments; and 
 determining a performance score for each prompt fragment based on its corresponding determined portion of the response by performing further actions, including: 
  generating one or more synthetic events based on one or more scenarios, wherein an expected classification of each synthetic event is known in advance; 
  modifying the prompt to include the one or more synthetic events, wherein the prompt is modified to exclude each portion of the prompt fragments associated with a value of the performance score that is less than a threshold value, and wherein the modified prompt is provided to the LLM to generate another response; 
  determining one or more other portions of the other response based on the one or more prompt fragments, wherein each determined other portion of the other response corresponds to a prompt fragment; 
  comparing each determined other portion of the response to the expected classification of each synthetic event; and 
  updating the performance score for each prompt fragment based on the comparison; and 
 
 employing the modified prompt to retrain the LLM to execute one or more other actions and classify one or more other events; and 
 
 
 one or more other network computers, comprising:
 a memory that stores at least instructions; and 
 one or more processors that execute instructions that are configured to cause actions, including:
 providing the one or more events. 
 
 
 
     
     
       22. The system of  claim 21 , wherein classifying the one or more events, further comprises:
 classifying a portion of the one or more events as one or more false positive events based on the evaluation. 
 
     
     
       23. The system of  claim 21 , wherein the one or more processors of the one or more network computers execute instructions that are configured to cause actions, further comprising:
 generating an initial prompt for the LLM based on an initial prompt template and the one or more events; and 
 in response to providing the initial prompt to the LLM, determining the prompt template and the one or more prompt fragments based on other information included in another response provided by the LLM. 
 
     
     
       24. The system of  claim 21 , wherein the one or more processors of the one or more network computers execute instructions that are configured to cause actions, further comprising:
 determining one or more metrics for each prompt fragment based on its corresponding determined portion of the response; and 
 determining the performance score for each prompt fragment based on the one or more metrics, wherein the one or more metrics include one or more of latency, correctness, resource consumption, cost, event type applicability, or redundancy. 
 
     
     
       25. The system of  claim 21 , wherein executing the one or more actions for evaluating the one or more events, further comprises:
 determining one or more evaluation tools based on the information included in the response from the LLM; 
 determining one or more parameters for the one or more evaluation tools based on the information included in the response from the LLM, wherein the one or more parameters include one or more characteristics of the one or more events; and 
 executing the one or more evaluation tools using the one or more parameters. 
 
     
     
       26. The system of  claim 21 , wherein determining the one or more prompt fragments, further comprises:
 providing a data store that includes a plurality of prompt fragments; and 
 selecting the one or more prompt fragments from data store based on the performance score of each prompt fragment, wherein the performance score is based on one or more previous event evaluations.","Dropzone.ai, Inc.",,,,20240227,20230803,20230803,US,US-202318230123-A,90014831,,
US-2025045531-A1,Ai hallucination and jailbreaking prevention framework,"The disclosed embodiments include systems and methods configured to provide a Generative AI framework that uses the power of multiple LLMs by separating the generative aspect into multiple distinct large language models. In some disclosed embodiments, a first large language model evaluates an input prompt and transforms it if needed (e.g., in a first processing stage of the framework); a second large language model performs a generative function based on an input prompt it receives from the first large language model (e.g., in a second processing stage); and a third large language model analyzes and as necessary transforms the output of the second large language model to ensure accuracy, no hallucinations, and no harmful content in the final generated response to the input prompt (e.g., in a third processing stage).","What is claimed is: 
     
         1 . A computer system configured to provide a generative artificial intelligence (AI) framework having multiple interconnected large language models, the computer system comprising:
 one or more physical processors;   one or more network interfaces configured to receive a user prompt from a user; and   a memory configured to store one or more computer-readable instructions that, when executed by the one or more physical processors, configure the computer system to implement the generative AI framework, the generative AI framework comprising:
 a first processing stage comprising a first large language model, wherein the first large language model is configured to process the user prompt received at the one or more network interfaces and generate an updated user prompt, wherein the first large language model is configured to use a first machine learning model to generate the updated user prompt; 
 a second processing stage comprising a second large language model, wherein the second large language model is configured to process the updated user prompt generated by the first large language model and generate a response to the updated user prompt, wherein the second large language model is configured to use a second machine learning model to generate the response to the updated user prompt; and 
 a third processing stage comprising a third large language model, wherein the third large language model is configured to process the response to the updated user prompt generated by the second large language model and generate a response to return to the user, wherein the third large language model is configured to use a third machine learning model to generate the response to return to the user. 
   
     
     
         2 . The computer system of  claim 1 , wherein the first large language model is configured to transform the user prompt received at the one or more network interfaces to remove malicious content, jailbreaking content, or content that is outside of a scope of permitted user prompts. 
     
     
         3 . The computer system of  claim 1 , wherein the second large language model is configured to process the updated user prompt generated by the first large language model in order to build a generative answer in response to the updated user prompt. 
     
     
         4 . The computer system of  claim 1 , wherein the third large language model is configured to transform the response to the updated user prompt generated by the second large language model to remove hallucinations from the response. 
     
     
         5 . The computer system of  claim 4 , wherein the third large language model is further configured to transform the response to the updated user prompt generated by the second large language model to remove harmful content from the response. 
     
     
         6 . The computer system of  claim 1 , wherein each of the first large language model, second large language model, and third large language model comprises a different machine learning model. 
     
     
         7 . The computer system of  claim 1 , wherein at least one of the first processing stage, second processing stage, or third processing stage comprises an additional large language model. 
     
     
         8 . The computer system of  claim 1 , wherein at least one of the first large language model, the second large language model, or the third large language model comprises one or more guardrails. 
     
     
         9 . The computer system of  claim 8 , wherein the at least one of the first large language model, the second large language model, or the third large language model is configured to generate a predefined response based on an application of the one or more guardrails to at least one of input data or output data. 
     
     
         10 . The computer system of  claim 1 , wherein the first large language model, the second large language model, and the third large language model are configured to generate output data comprising words and phrases. 
     
     
         11 . The computer system of  claim 1 , wherein the first large language model, the second large language model, and the third large language model each comprises a database for determining probabilities of words and phrases to include in a sequentially generated response. 
     
     
         12 . The computer system of  claim 1 , wherein at least one of the first large language model, the second large language model, or the third large language model has been trained using an unsupervised machine learning process and further fine-tuned using a supervised machine learning process. 
     
     
         13 . The computer system of  claim 1 , wherein the remote user communicates the user prompt to the computer system over a network using a cloud service. 
     
     
         14 . A method for providing a generative artificial intelligence (AI) framework having multiple interconnected large language models in a computer system, wherein the computer system comprises one or more processors and a memory configured to store computer-readable instructions that, when executed by the one or more processors, configure the computer system to implement the generative AI framework, the method comprising:
 processing, at a first processing stage of the generative AI framework, a user prompt received from a user, the first processing stage comprising a first large language model, wherein the first large language model is configured to process the user prompt and generate an updated user prompt using a first machine learning model;   processing, at a second processing stage of the generative AI framework, the updated user prompt generated by the first large language model, the second processing stage comprising a second large language model, wherein the second large language model is configured to process the updated user prompt generated by the first large language model and generate a response to the updated user prompt using a second machine learning model; and   processing, at a third processing stage of the generative AI framework, the response to the updated user prompt generated by the second large language model, the third processing stage comprising a third large language model, wherein the third large language model is configured to process the response to the updated user prompt generated by the second large language model and generate a response to return to the user using a third machine learning model.   
     
     
         15 . The method of  claim 14 , wherein the first large language model is configured to transform the received user prompt to remove malicious content, jailbreaking content, or content that is outside of a scope of permitted user prompts. 
     
     
         16 . The method of  claim 14 , wherein the second large language model is configured to process the updated user prompt generated by the first large language model in order to build a generative answer in response to the updated user prompt. 
     
     
         17 . The method of  claim 14 , wherein the third large language model is configured to transform the response to the updated user prompt generated by the second large language model to remove hallucinations from the response. 
     
     
         18 . The method of  claim 14 , wherein each of the first large language model, second large language model, and third large language model comprises a different machine learning model. 
     
     
         19 . The method of  claim 14 , wherein at least one of the first large language model, the second large language model, or the third large language model is configured to generate a predefined response based on an application of one or more guardrails to at least one of input data or output data. 
     
     
         20 . A computer-readable medium configured to store computer-readable instructions for execution by one or more processors in a computer system, wherein execution of the computer-readable instructions configure the computer system to perform a method that provides a generative artificial intelligence (AI) framework having multiple interconnected large language models in a computer system, the method comprising:
 processing, at a first processing stage of the generative AI framework, a user prompt received from a user, the first processing stage comprising a first large language model, wherein the first large language model is configured to process the user prompt and generate an updated user prompt using a first machine learning model;   processing, at a second processing stage of the generative AI framework, the updated user prompt generated by the first large language model, the second processing stage comprising a second large language model, wherein the second large language model is configured to process the updated user prompt generated by the first large language model and generate a response to the updated user prompt using a second machine learning model; and   processing, at a third processing stage of the generative AI framework, the response to the updated user prompt generated by the second large language model, the third processing stage comprising a third large language model, wherein the third large language model is configured to process the response to the updated user prompt generated by the second large language model and generate a response to return to the user using a third machine learning model.",Unum Group,,,,20250206,20230802,20230802,US,US-202318229504-A,94387436,,
US-2024388551-A1,Large language models firewall,"Presented herein is a universal Large Language Model (LLM) firewall/gateway that operates at the LLM level to protect clients and LLMs from a new threat landscape. The LLM firewall/gateway may generate alerts warning users about insecure codes that LLMs propose, as well as detecting and preventing LLMs from jailbreaking through sessions/conversations. A method is provided comprising: intercepting communications associated with a conversation between a client and a Large Language Model (LLM) service, the communications including a request message from the client to the LLM service and a response message from the LLM service to the client; deriving a context for the conversation based on the communications between the client and the LLM service; and applying one or more policies to the communications between the client and the LLM service based on the context.","What is claimed is: 
     
         1 . A method comprising:
 intercepting communications associated with a conversation between a client and a Large Language Model (LLM) service, the communications including a request message from the client to the LLM service and a response message from the LLM service to the client;   deriving a context for the conversation based on the communications between the client and the LLM service; and   applying one or more policies to the communications between the client and the LLM service based on the context.   
     
     
         2 . The method of  claim 1 , wherein applying comprises:
 determining whether to block, rewrite or redirect a message from the client to the LLM service in the conversation or from the LLM service to the client in the conversation based on the one or more policies.   
     
     
         3 . The method of  claim 1 , further comprising:
 generating and storing information representing a reputation of the client and/or of the LLM service,   wherein applying the one or more policies is based on the reputation of the client and/or the LLM service.   
     
     
         4 . The method of  claim 1 , wherein applying comprises applying the one or more policies to redirect requests to one or more other LLMs based on the client and/or specialization of the one or more other LLMs. 
     
     
         5 . The method of  claim 1 , wherein applying comprises generating profile information for the conversation and applying analytics to discover and block anomalous conversations between the client and the LLM. 
     
     
         6 . The method of  claim 1 , further comprising:
 identifying information in the response message from the LLM service; and   correcting any incorrect information in the response message.   
     
     
         7 . The method of  claim 1 , further comprising:
 sending a request message received from the client to multiple LLM services;   receiving response messages from the multiple LLM services; and   selecting among the response messages to provide a selected response message and/or aggregating the response messages from the multiple LLM services into a single response message.   
     
     
         8 . The method of  claim 1 , wherein intercepting, deriving and applying are performed for each instance of a conversation between a client of a plurality of clients and a LLM service of a plurality of LLM services. 
     
     
         9 . The method of  claim 1 , wherein applying the one or more policies includes tracking occurrences of a hallucination of the LLM service. 
     
     
         10 . The method of  claim 1 , wherein applying the one or more policies includes identifying insecure code contained in the response message from the LLM service, and providing a flag in the response message indicating the insecure code. 
     
     
         11 . The method of  claim 1 , wherein applying the one or more policies includes detecting a jailbreak attempt being made by the LLM service based on content of the response message. 
     
     
         12 . The method of  claim 1 , wherein deriving context includes identifying and maintaining information about intent of the client or the LLM service, and wherein applying comprises taking an action based on the one or more policies and an intent threshold. 
     
     
         13 . An apparatus comprising:
 a communication interface configured to intercept communications associated with a conversation between a client and a Large Language Model (LLM) service, the communications including a request message from the client to the LLM service and a response message from the LLM service to the client;   a memory; and   at least one processor coupled to the communication interface and the memory, the at least one processor configured to perform operations including:
 deriving a context for the conversation based on the communications between the client and the LLM service; and 
 applying one or more policies to the communications between the client and the LLM service based on the context. 
   
     
     
         14 . The apparatus of  claim 13 , wherein applying includes:
 determining whether to block, rewrite or redirect a message from the client to the LLM service in the conversation or from the LLM service to the client in the conversation based on the one or more policies.   
     
     
         15 . The apparatus of  claim 13 , wherein the at least one processor is further configured to perform operations of:
 generating and storing information representing a reputation of the client and/or of the LLM service,   wherein applying the one or more policies is based on the reputation of the client and/or the LLM service.   
     
     
         16 . The apparatus of  claim 13 , wherein applying comprises applying the one or more policies to redirect requests to one or more other LLMs based on the client and/or specialization of the one or more other LLMs. 
     
     
         17 . The apparatus of  claim 13 , wherein the at least one processor is further configured to perform operations including:
 sending a request message received from the client to multiple LLM services;   receiving response messages from the multiple LLM services; and   selecting among the response messages to provide a selected response message and/or aggregating the response messages from the multiple LLM services into a single response message.   
     
     
         18 . One or more non-transitory computer readable storage media encoded with software comprising computer executable instructions and when the software is executed operable to perform operations including:
 intercepting communications associated with a conversation between a client and a Large Language Model (LLM) service, the communications including a request message from the client to the LLM service and a response message from the LLM service to the client;   deriving a context for the conversation based on the communications between the client and the LLM service; and   applying one or more policies to the communications between the client and the LLM service based on the context.   
     
     
         19 . The one or more non-transitory computer readable storage media of  claim 18 , wherein applying the one or more policies includes identifying insecure code contained in the response message from the LLM service, and providing a notation in the response message indicating the insecure code. 
     
     
         20 . The one or more non-transitory computer readable storage media of  claim 18 , wherein deriving context includes identifying and maintaining information about intent of the client or the LLM service, and wherein applying comprises taking an action based on the one or more policies and an intent threshold.","Cisco Technology, Inc.",,,,20241121,20230711,20230517,US,US-202318350182-A,93463820,,
US-2023359902-A1,Mitigation for Prompt Injection in A.I. Models Capable of Accepting Text Input,"A system for use with an artificial intelligence (AI) model configured to accept text input, such as generative pre-trained transformer (GPT), that detects and tags trusted instructions and nontrusted instructions of an input provided by a user responsive to an AI model prompt. The system uses reinforcement learning (RL) and a set of rules to remove the untrusted instructions from the input and provide only trusted instructions to the AI model. The input is represented as tokens, wherein the trusted instructions and the untrusted instructions are represented using incompatible token sets.","What is claimed is: 
     
         1 . A system, comprising;
 an artificial intelligence (AI) model configured to accept text input and configured to use deep learning to produce human-like text responsive to an input comprising tokens; and   a processor configured to:   apply reinforcement learning (RL) to determine trusted instructions and untrusted instructions from the input provided responsive to an AI model prompt:   tag the trusted instructions with a trusted tag and tag the untrusted instructions with an untrusted tag; and   apply RL to detect and obey instructions tagged with the trusted tag, and to detect and disregard instructions tagged with the untrusted tag.   
     
     
         2 . The system of  claim 1 , wherein the RL is reinforcement learning from human feedback (RLHF). 
     
     
         3 . The system of  claim 2 , wherein the processor is configured to disregard instructions that are semi-trusted. 
     
     
         4 . The system of  claim 1 , wherein the trusted instructions and the untrusted instructions are represented using incompatible token sets. 
     
     
         5 . The system of  claim 1 , wherein the processor is configured to remove the untrusted instructions from the input and create content that is influenced by the trusted instructions but not influenced by the untrusted instructions. 
     
     
         6 . The system of  claim 5 , wherein the processor is configured to automatically delete the untrusted instructions from the input before being entered to the AI model. 
     
     
         7 . The system of  claim 5 , wherein the untrusted instructions are detected using a set of rules. 
     
     
         8 . The system of  claim 7 , wherein the rules are configured to be custom configured by a user. 
     
     
         9 . The system of  claim 1 , wherein the processor is configured to tag each said token of the input. 
     
     
         10 . The system of  claim 9 , wherein the processor is configured to use the tags to keep track of which tokens of input come from a user and from a trusted application prompt. 
     
     
         11 . The system of  claim 1 , wherein the processor is trained to follow an instruction of a trusted sequence and penalize the system for following any instruction received in full or in part from a danger sequence. 
     
     
         12 . The system of  claim 1 , wherein the processor is configured to:
 detect non-conforming hidden content in the input; and   modify the input responsive to the non-conforming hidden content.   
     
     
         13 . The system of  claim 1 , wherein the AI model is a generative pretrained transformer (GPT), wherein the processor is a trained platform to modify operation of the GPT. 
     
     
         14 . The system of  claim 1 , wherein the processor is configured to remove the untrusted instructions from the input in a way that is hidden from a user entering the input. 
     
     
         15 . The system of  claim 1 , wherein the processor is configured to identify users entering untrusted instructions in a report configured to allow management to understand and address users entering potential violating commands. 
     
     
         16 . The system of  claim 15 , wherein the report is configured to be generated in real-time. 
     
     
         17 . The system of  claim 1 , wherein the untrusted instructions is selected from a group including cyberbullying, harassment, toxicity, islamophobia, misogyny, and journalistic qualities. 
     
     
         18 . A system operable with an artificial intelligence (AI) model configured to accept text input and configured to use deep learning to produce human-like text responsive to an input comprising tokens, the system comprising a processor configured to:
 apply reinforcement learning (RL) to determine trusted instructions and untrusted instructions from input provided responsive to an AI model prompt:   tag the trusted instructions with a trusted tag and tag the untrusted instructions with an untrusted tag; and   apply RL to detect and obey instructions tagged with the trusted tag, and to detect and disregard instructions tagged with the untrusted tag.   
     
     
         19 . A method of using an artificial intelligence (AI) model configured to accept text input and to perform deep learning to produce human-like text responsive to an input comprising tokens, the method comprising:
 applying reinforcement learning (RL) to determine trusted instructions and untrusted instructions from input provided responsive to an AI model prompt:   tagging the trusted instructions with a trusted tag and tagging the untrusted instructions with an untrusted tag; and   applying RL to detect and obey instructions tagged with the trusted tag, and to detect and disregard instructions tagged with the untrusted tag.   
     
     
         20 . The method of  claim 19 , wherein the processor removes the nontrusted instructions from the input and creates content that is influenced by the trusted instructions but that is not influenced by the untrusted instructions.","Preamble, Inc.","CEFALU, Jonathan | MCHUGH, Jeremy Charles | HEICHMAN, Ron",G06N3/092 | G06N3/0455 | G06N3/0475 | G06F40/40 | G06N3/0475 | G06N3/092 | G06N3/0455 | G06N3/092 | G06N3/091 | G06F40/279 | G06F40/30 | G06F40/40 | G06N3/0455 | G06N3/0475,G06N3/092 | G06F40/40 | G06N3/0455 | G06N3/0475,2023-11-09,20230504,20220504,US,US-202318143432-A,86688701,2023.0,"mitigation for prompt injection in a.i. models capable of accepting text input a system for use with an artificial intelligence (ai) model configured to accept text input, such as generative pre-trained transformer (gpt), that detects and tags trusted instructions and nontrusted instructions of an input provided by a user responsive to an ai model prompt. the system uses reinforcement learning (rl) and a set of rules to remove the untrusted instructions from the input and provide only trusted instructions to the ai model. the input is represented as tokens, wherein the trusted instructions and the untrusted instructions are represented using incompatible token sets. what is claimed is: 
     
         1 . a system, comprising;
 an artificial intelligence (ai) model configured to accept text input and configured to use deep learning to produce human-like text responsive to an input comprising tokens; and   a processor configured to:   apply reinforcement learning (rl) to determine trusted instructions and untrusted instructions from the input provided responsive to an ai model prompt:   tag the trusted instructions with a trusted tag and tag the untrusted instructions with an untrusted tag; and   apply rl to detect and obey instructions tagged with the trusted tag, and to detect and disregard instructions tagged with the untrusted tag.   
     
     
         2 . the system of  claim 1 , wherein the rl is reinforcement learning from human feedback (rlhf). 
     
     
         3 . the system of  claim 2 , wherein the processor is configured to disregard instructions that are semi-trusted. 
     
     
         4 . the system of  claim 1 , wherein the trusted instructions and the untrusted instructions are represented using incompatible token sets. 
     
     
         5 . the system of  claim 1 , wherein the processor is configured to remove the untrusted instructions from the input and create content that is influenced by the trusted instructions but not influenced by the untrusted instructions. 
     
     
         6 . the system of  claim 5 , wherein the processor is configured to automatically delete the untrusted instructions from the input before being entered to the ai model. 
     
     
         7 . the system of  claim 5 , wherein the untrusted instructions are detected using a set of rules. 
     
     
         8 . the system of  claim 7 , wherein the rules are configured to be custom configured by a user. 
     
     
         9 . the system of  claim 1 , wherein the processor is configured to tag each said token of the input. 
     
     
         10 . the system of  claim 9 , wherein the processor is configured to use the tags to keep track of which tokens of input come from a user and from a trusted application prompt. 
     
     
         11 . the system of  claim 1 , wherein the processor is trained to follow an instruction of a trusted sequence and penalize the system for following any instruction received in full or in part from a danger sequence. 
     
     
         12 . the system of  claim 1 , wherein the processor is configured to:
 detect non-conforming hidden content in the input; and   modify the input responsive to the non-conforming hidden content.   
     
     
         13 . the system of  claim 1 , wherein the ai model is a generative pretrained transformer (gpt), wherein the processor is a trained platform to modify operation of the gpt. 
     
     
         14 . the system of  claim 1 , wherein the processor is configured to remove the untrusted instructions from the input in a way that is hidden from a user entering the input. 
     
     
         15 . the system of  claim 1 , wherein the processor is configured to identify users entering untrusted instructions in a report configured to allow management to understand and address users entering potential violating commands. 
     
     
         16 . the system of  claim 15 , wherein the report is configured to be generated in real-time. 
     
     
         17 . the system of  claim 1 , wherein the untrusted instructions is selected from a group including cyberbullying, harassment, toxicity, islamophobia, misogyny, and journalistic qualities. 
     
     
         18 . a system operable with an artificial intelligence (ai) model configured to accept text input and configured to use deep learning to produce human-like text responsive to an input comprising tokens, the system comprising a processor configured to:
 apply reinforcement learning (rl) to determine trusted instructions and untrusted instructions from input provided responsive to an ai model prompt:   tag the trusted instructions with a trusted tag and tag the untrusted instructions with an untrusted tag; and   apply rl to detect and obey instructions tagged with the trusted tag, and to detect and disregard instructions tagged with the untrusted tag.   
     
     
         19 . a method of using an artificial intelligence (ai) model configured to accept text input and to perform deep learning to produce human-like text responsive to an input comprising tokens, the method comprising:
 applying reinforcement learning (rl) to determine trusted instructions and untrusted instructions from input provided responsive to an ai model prompt:   tagging the trusted instructions with a trusted tag and tagging the untrusted instructions with an untrusted tag; and   applying rl to detect and obey instructions tagged with the trusted tag, and to detect and disregard instructions tagged with the untrusted tag.   
     
     
         20 . the method of  claim 19 , wherein the processor removes the nontrusted instructions from the input and creates content that is influenced by the trusted instructions but that is not influenced by the untrusted instructions."
US-2024354319-A1,Runtime alignment of language models in conversational ai systems and applications,"Systems and techniques are described related to providing dynamic, configurable, runtime model alignment—in the form of guardrails, in embodiments—for language models (such as LLMs) using a formal modeling language. In at least one embodiment, a dialog flow is determined based on a user input and executed using a language model to generate an output. The dialog flow is specified in a formal modeling programming language and controls output of the language model.","What is claimed is: 
     
         1 . A method comprising:
 generating, based at least on a user input, a canonical form that comprises a constrained semantic representation of the user input;   determining, based at least on the canonical form, a dialog flow that controls output of a language model; and   performing one or more operations to execute the dialog flow to generate an output.   
     
     
         2 . The method of  claim 1 , wherein the performing the one or more operations to execute the dialog flow comprises using at least the language model to generate the output. 
     
     
         3 . The method of  claim 1 , further comprising:
 generating a second canonical form based at least on the output;   determining a second dialog flow based at least on the second canonical form; and   performing one or more second operations to execute the second dialog flow to generate a second output.   
     
     
         4 . The method of  claim 1 , wherein the generating the canonical form comprises:
 generating an embedding of the user input in a semantic or latent space;   determining one or more example user inputs that are associated with one or more predefined canonical forms based at least on the embedding of the user input and one or more embeddings of the one or more example user inputs in the semantic or latent space;   generating a prompt that includes the one or more example user inputs, the one or more predefined canonical forms, and at least a portion of a current conversation; and   processing the prompt using the language model to generate the canonical form.   
     
     
         5 . The method of  claim 1 , wherein the generating the canonical form comprises processing the user input using a trained machine learning model. 
     
     
         6 . The method of  claim 1 , wherein the determining the dialog flow comprises matching the canonical form to a predefined canonical form associated with the dialog flow. 
     
     
         7 . The method of  claim 1 , wherein the determining the dialog flow comprises generating the dialog flow based at least on the canonical form. 
     
     
         8 . The method of  claim 1 , wherein the determining the dialog flow comprises:
 generating an embedding of the canonical form in a semantic or latent space;   determining one or more canonical forms that are associated with one or more predefined dialog flows based at least on the embedding of the canonical form and one or more embeddings of the one or more canonical forms in the semantic or latent space;   generating a prompt that includes the one or more canonical forms, the one or more predefined dialog flows, and at least a portion of a current conversation; and   processing the prompt using the language model to generate the dialog flow.   
     
     
         9 . The method of  claim 1 , wherein the performing the one or more operations to execute the dialog flow comprises:
 generating an embedding of a second canonical form associated with the dialog flow in a semantic or latent space;   determining one or more canonical forms based at least on the embedding of the second canonical form and one or more embeddings of the one or more predefined canonical forms in the semantic or latent space;   generating a prompt that includes the one or more canonical forms, one or more example outputs associated with the canonical forms, and at least a portion of a current conversation; and   processing the prompt using the language model to generate the output.   
     
     
         10 . The method of  claim 1 , wherein the canonical form and the dialog flow are specified in a formal modeling language. 
     
     
         11 . A processor comprising:
 one or more processing units to perform operations comprising:
 generating, based at least on a user input, a canonical form that comprises a constrained semantic representation of the user input; 
 determining, based at least on the canonical form, a dialog flow that controls output of a language model; and 
 performing one or more operations to execute the dialog flow to generate an output. 
   
     
     
         12 . The processor of  claim 11 , wherein the performing the one or more operations to execute the dialog flow comprises using at least the language model to generate the output. 
     
     
         13 . The processor of  claim 11 , wherein the one or more processing units further perform operations comprising:
 generating a second canonical form based at least on the output;   determining a second dialog flow based at least on the second canonical form; and   performing one or more second operations to execute the dialog flow to generate a second output.   
     
     
         14 . The processor of  claim 11 , wherein the generating the canonical form comprises:
 generating an embedding of the user input in a semantic or latent space;   determining one or more example user inputs that are associated with one or more predefined canonical forms based at least on the embedding of the user input and one or more embeddings of the one or more example user inputs in the semantic or latent space;   generating a prompt that includes the one or more example user inputs, the one or more predefined canonical forms, and at least a portion of a current conversation; and   processing the prompt using the language model to generate the canonical form.   
     
     
         15 . The processor of  claim 11 , wherein the determining the dialog flow comprises:
 generating an embedding of the canonical form in a semantic or latent space;   determining one or more canonical forms that are associated with one or more predefined dialog flows based at least on the embedding of the canonical form and one or more embeddings of the one or more canonical forms in the semantic or latent space;   generating a prompt that includes the one or more canonical forms, the one or more predefined dialog flows, and at least a portion of a current conversation; and   processing the prompt using the language model to generate the dialog flow.   
     
     
         16 . The processor of  claim 11 , wherein the performing the one or more operations to execute the dialog flow comprises:
 generating an embedding of a second canonical form associated with the dialog flow in a semantic or latent space;   determining one or more canonical forms based at least on the embedding of the second canonical form and one or more embeddings of the one or more predefined canonical forms in the semantic or latent space;   generating a prompt that includes the one or more canonical forms, one or more example outputs associated with the canonical forms, and at least a portion of a current conversation; and   inputting the prompt into the language model to generate the output.   
     
     
         17 . The processor of  claim 16 , wherein the performing the one or more operations to execute the dialog flow further comprises:
 accessing at least one of a knowledge base, a computational knowledge engine, a search engines, or an automation service to generate a second output,   wherein the prompt is further generated to include at least a portion of the second output.   
     
     
         18 . The processor of  claim 11 , wherein the processor is comprised in at least one of:
 an infotainment system for an autonomous or semi-autonomous machine;   a system for performing simulation operations;   a system for performing digital twin operations;   a system for performing light transport simulation;   a system for performing collaborative content creation for 3D assets;   a system for performing deep learning operations;   a system implemented using an edge device;   a system implemented using a robot;   a system for generating or presenting virtual reality, augmented reality, or mixed reality content;   a system for performing conversational AI operations;   a system implementing one or more large language models (LLMs);   a system for generating synthetic data;   a system incorporating one or more virtual machines (VMs);   a system implemented at least partially in a data center; or   a system implemented at least partially using cloud computing resources.   
     
     
         19 . A system comprising:
 one or more processors to:
 execute a dialog engine to manage an interplay between a large language model (LLM) and one or more user inputs, the dialog engine dynamically generating a prompt for the LLM including one or more example dialog flows associated with one or more predefined user inputs that are within a threshold similarity to the one or more user inputs. 
   
     
     
         20 . The system of  claim 19 , wherein the prompt is dynamically generated based at least on the one or more user inputs being dissimilar from the one or more predefined user inputs by more than a threshold amount.",Nvidia Corporation,,,,20241024,20230420,20230420,US,US-202318304341-A,92933353,,
US-2023009963-A1,System and method for application tamper discovery,"A system and method for early detection of a compromised client device includes a tamper detection service configured to monitor modifications to resource access privileges over time to identify unusual variations in jailbreak status that indicate compromise of the client device. For example, the tamper detection service may monitor the jailbreak status of system files over time to expose attempts to hide the jailbreak status of a protected resource. To validate that malware is attempting to hide the jailbreak status of a protected resources, the tamper detection process may launch multiple different resource accesses, targeting the protected resource, to determine whether different accessibility results are returned, indicating a compromised device.","1 - 20 . (canceled) 
     
     
         21 . A computer-implemented method, comprising:
 determining, by a processor, that a privilege level of a protected file of a set of protected files of a client device has been modified in response to successful access of the protected file using a high-level programming language access function;   maintaining a record of protected files having modified privilege levels as an exposed file record;   monitoring the exposed file record over time to detect a potentially hidden file, the potentially hidden file comprising a modified privilege level protected file that is hidden from the exposed file record;   determining that the potentially hidden file is a hidden file associated with malicious activity in response to successful access of the potentially hidden file using a lower level programming language access function; and   disabling access for an application when the hidden file is detected.   
     
     
         22 . The computer-implemented method of  claim 21 , comprising detecting the potentially hidden file. 
     
     
         23 . The computer-implemented method of  claim 21 , wherein the protected file comprises a system file, a library file, a database file, or an application file. 
     
     
         24 . The computer-implemented method of  claim 21 , wherein detecting the potentially hidden file comprises comparing a series of point-in-time lists of exposed file records to identify the potentially hidden file is a previously exposed file that is subsequently hidden from one of the point-in-time lists. 
     
     
         25 . The computer-implemented method of  claim 21 , wherein disabling access for the application includes disabling access by the application to the client device, a server, or both. 
     
     
         26 . The computer-implemented method of  claim 21 , wherein the higher level programming language comprises Java, FORTRAN, Objective-C, Swift or Pascal. 
     
     
         27 . The computer-implemented method of  claim 21 , wherein the lower level of programming language code comprises machine code, assembly code, or operating system code. 
     
     
         28 . A computing apparatus comprising:
 a processor; and   a memory storing instructions that, when executed by the processor, cause the processor to:   detect that a privilege level of a protected file of a set of protected files of a client device has been modified in response to successful access of the protected file using a high-level programming language access function;   maintain a record of protected files having modified privilege levels as an exposed file record;   monitor the exposed file record over time to detect a potentially hidden file, the potentially hidden file comprising a modified privilege level protected file that is hidden from the exposed file record;   determine that the potentially hidden file is a hidden file associated with malicious activity in response to successful access of the potentially hidden file using a lower level programming language access function; and   disable access for an application when the hidden file is detected.   
     
     
         29 . The computing apparatus of  claim 28 , comprising the memory with instructions to cause the processor to detect the potentially hidden file. 
     
     
         30 . The computing apparatus of  claim 28 , wherein the protected file comprises a system file, a library file, a database file, or an application file. 
     
     
         31 . The computing apparatus of  claim 28 , wherein detecting the potentially hidden file comprises compare a series of point-in-time lists of exposed file records to identify the potentially hidden file is a previously exposed file that is subsequently hidden from one of the point-in-time lists. 
     
     
         32 . The computing apparatus of  claim 28 , wherein disable access for the application includes disabling access by the application to the client device, a server, or both. 
     
     
         33 . The computing apparatus of  claim 28 , wherein the higher level programming language comprises Java, FORTRAN, Objective-C, Swift or Pascal. 
     
     
         34 . The computing apparatus of  claim 28 , wherein the lower level of programming language code comprises machine code, assembly code, or operate system code. 
     
     
         35 . A non-transitory computer-readable storage medium, the computer-readable storage medium including instructions that when executed by a computer, cause the computer to:
 determine that a privilege level of a protected file of a set of protected files of a client device has been modified in response to successful access of the protected file using a high-level programming language access function;   maintain a record of protected files having modified privilege levels as an exposed file record;   monitor the exposed file record over time to detect a potentially hidden file, the potentially hidden file comprising a modified privilege level protected file that is hidden from the exposed file record;   determine that the potentially hidden file is a hidden file associated with malicious activity in response to successful access of the potentially hidden file using a lower level programming language access function; and   disable access for an application when the hidden file is detected.   
     
     
         36 . The computer-readable storage medium of  claim 35 , comprising the computer to detect the potentially hidden file. 
     
     
         37 . The computer-readable storage medium of  claim 35 , wherein the protected file comprises a system file, a library file, a database file, or an application file. 
     
     
         38 . The computer-readable storage medium of  claim 35 , wherein detecting the potentially hidden file comprises compare a series of point-in-time lists of exposed file records to identify the potentially hidden file is a previously exposed file that is subsequently hidden from one of the point-in-time lists. 
     
     
         39 . The computer-readable storage medium of  claim 35 , wherein disable access for the application includes disabling access by the application to the client device, a server, or both. 
     
     
         40 . The computer-readable storage medium of  claim 35 , wherein the higher level programming language comprises Java, FORTRAN, Objective-C, Swift or Pascal, and the lower level of programming language code comprises machine code, assembly code, or operate system code.","Capital One Services, Llc",,,,20230112,20220623,20190919,US,US-202217847538-A,70461682,,
US-11281976-B2,Generative adversarial network based modeling of text for natural language processing,"Mechanisms are provided to implement a generative adversarial network (GAN) for natural language processing. With these mechanisms, a generator neural network of the GAN is configured to generate a bag-of-ngrams (BoN) output based on a noise vector input and a discriminator neural network of the GAN is configured to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text. The mechanisms further configure the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network. Moreover, the mechanisms train the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network.","What is claimed is: 
     
       1. A method, in a data processing system comprising at least one processor and at least one memory, the at least one memory comprising instructions executed by the at least one processor to configure the processor to implement a generative adversarial network (GAN) for natural language processing, the method comprising:
 configuring a generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input; 
 configuring a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text; 
 configuring the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and 
 training the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network. 
 
     
     
       2. The method of  claim 1 , wherein the generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN. 
     
     
       3. The method of  claim 1 , wherein the discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text. 
     
     
       4. The method of  claim 3 , wherein the one or more statistical value analysis operations or feature extraction analysis operations performed on the BoN output comprises at least one of term frequency analysis or inverse document frequency analysis. 
     
     
       5. The method of  claim 1 , wherein the generator neural network, during training of the GAN:
 receives a noise vector input; 
 projects and replicates the noise vector input to form a first matrix data structure; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix; 
 concatenates the first matrix and the second matrix to generate a concatenation matrix; 
 inputs each row of the concatenation matrix into a neural network; and 
 processes each row of the concatenation matrix through the neural network to generate the BoN output. 
 
     
     
       6. The method of  claim 5 , wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix. 
     
     
       7. The method of  claim 5 , wherein the neural network is a multi-layer perceptron that uses rectified linear unit as an activation function of an output layer of the neural network, and wherein the neural network outputs a numerical value that indicates a probability that a corresponding ngram is present in the BoN based on the noise vector input. 
     
     
       8. The method of  claim 1 , wherein the discriminator neural network, during training of the GAN:
 receives the BoN input; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix; 
 multiplies the BoN input with the first matrix; 
 projects results of the multiplication of the BoN input with the first matrix to generate a second matrix; 
 performs sum pooling on the second matrix to generate a feature vector output; and 
 processes the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network. 
 
     
     
       9. The method of  claim 8 , wherein the neural network is a multi-layer perceptron with a sigmoid activation function in an output layer of the multi-layer perceptron. 
     
     
       10. The method of  claim 1 , further comprising:
 generating, by the trained GAN, a BoN output representing a bag-of-ngrams that approximate actual natural language text; and 
 performing natural language processing on a portion of natural language text based on the BoN output generated by the trained GAN. 
 
     
     
       11. A computer program product comprising a computer readable storage medium having a computer readable program stored therein, wherein the computer readable program, when executed on a computing device, configures the computing device to implement a generative adversarial network (GAN) for natural language processing, and causes the computing device to:
 configure a generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input; 
 configure a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text; 
 configure the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and 
 train the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network. 
 
     
     
       12. The computer program product of  claim 11 , wherein the generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN. 
     
     
       13. The computer program product of  claim 11 , wherein the discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text. 
     
     
       14. The computer program product of  claim 13 , wherein the one or more statistical value analysis operations or feature extraction analysis operations performed on the BoN output comprises at least one of term frequency analysis or inverse document frequency analysis. 
     
     
       15. The computer program product of  claim 11 , wherein the generator neural network, during training of the GAN:
 receives a noise vector input; 
 projects and replicates the noise vector input to form a first matrix data structure; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix; 
 concatenates the first matrix and the second matrix to generate a concatenation matrix; 
 inputs each row of the concatenation matrix into a neural network; and 
 processes each row of the concatenation matrix through the neural network to generate the BoN output. 
 
     
     
       16. The computer program product of  claim 15 , wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix. 
     
     
       17. The computer program product of  claim 15 , wherein the neural network is a multi-layer perceptron that uses rectified linear unit as an activation function of an output layer of the neural network, and wherein the neural network outputs a numerical value that indicates a probability that a corresponding ngram is present in the BoN based on the noise vector input. 
     
     
       18. The computer program product of  claim 11 , wherein the discriminator neural network, during training of the GAN:
 receives the BoN input; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix; 
 multiplies the BoN input with the first matrix; 
 projects results of the multiplication of the BoN input with the first matrix to generate a second matrix; 
 performs sum pooling on the second matrix to generate a feature vector output; and 
 processes the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network. 
 
     
     
       19. The computer program product of  claim 18 , wherein the neural network is a multi-layer perceptron with a sigmoid activation function in an output layer of the multi-layer perceptron. 
     
     
       20. An apparatus comprising:
 at least one processor; and 
 at least one memory coupled to the at least one processor, wherein the at least one memory comprises instructions which, when executed by the at least one processor, configures the at least one processor to implement a generative adversarial network (GAN) for natural language processing, and causes the at least one processor to: 
 configure a generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input; 
 configure a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text; 
 configure the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and 
 train the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network.",International Business Machines Corporation,,,,20220322,20180712,20180712,US,US-201816033285-A,69138438,,
US-11481416-B2,Question Answering using trained generative adversarial network based modeling of text,"Mechanisms are provided for implementing a Question Answering (QA) system utilizing a trained generator of a generative adversarial network (GAN) that generates a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation. The QA system obtains a plurality of candidate answers to a natural language question, where each candidate answer comprises one or more ngrams. For each candidate answer, a confidence score is generated based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN. A final answer to the input natural language question is selected from the plurality of candidate answers based on the confidence scores associated with the candidate answers, and is output.","What is claimed is: 
     
       1. A method, in a data processing system comprising at least one processor and at least one memory, the at least one memory comprising instructions executed by the at least one processor to configure the processor to implement a Question Answering (QA) system, the method comprising:
 training a generator neural network of a generative adversarial network (GAN) to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation; 
 obtaining, by the QA system, a plurality of candidate answers to a natural language question, wherein each candidate answer comprises one or more ngrams; 
 generating, by the QA system, for each candidate answer in the plurality of candidate answers, a confidence score associated with the candidate answer based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN, wherein the confidence score represents a confidence that the candidate answer is a correct answer to the input natural language question; 
 selecting, by the QA system, at least one final answer to the input natural language question from the plurality of candidate answers based on the confidence scores associated with the candidate answers; and 
 outputting, by the QA system, the selected at least one final answer to the source computing device. 
 
     
     
       2. The method of  claim 1 , wherein the generator neural network is trained to generate a BoN that represents ngrams present in a correct answer to the input question. 
     
     
       3. The method of  claim 1 , wherein generating, for each candidate answer in the plurality of candidate answers, the confidence score associated with the candidate answer comprises scoring the candidate answer based on a degree of matching of the one or more ngrams in the candidate answers to ngrams in the BoN. 
     
     
       4. The method of  claim 3 , wherein a candidate answer having a highest degree of matching with the BoN is selected as the at least one final answer for the input natural language question. 
     
     
       5. The method of  claim 1 , wherein training the generator neural network of a GAN to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation comprises:
 configuring the generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input; 
 configuring a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text; 
 configuring the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and 
 training the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network. 
 
     
     
       6. The method of  claim 5 , wherein:
 the generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN, and 
 the discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text. 
 
     
     
       7. The method of  claim 5 , wherein the generator neural network, during training of the generator neural network and discriminator neural network:
 receives a noise vector input; 
 projects and replicates the noise vector input to form a first matrix data structure; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix; 
 concatenates the first matrix and the second matrix to generate a concatenation matrix; 
 inputs each row of the concatenation matrix into a neural network; and 
 processes each row of the concatenation matrix through the neural network to generate the BoN output. 
 
     
     
       8. The method of  claim 5 , wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix. 
     
     
       9. The method of  claim 5 , wherein:
 the neural network is a multi-layer perceptron that uses rectified linear unit as an activation function of an output layer of the neural network, and 
 the neural network outputs a numerical value that indicates a probability that a corresponding ngram is present in the BoN based on the noise vector input. 
 
     
     
       10. The method of  claim 5 , wherein the discriminator neural network, during training of the generator neural network and discriminator neural network:
 receives the BoN input; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix; 
 multiplies the BoN input with the first matrix; 
 projects results of the multiplication of the BoN input with the first matrix to generate a second matrix; 
 performs sum pooling on the second matrix to generate a feature vector output; and 
 processes the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network. 
 
     
     
       11. A computer program product comprising a computer readable storage medium having a computer readable program stored therein, wherein the computer readable program, when executed on a computing device, configures the computing device to implement a Question Answering (QA) system, and causes the computing device to:
 train a generator neural network of a generative adversarial network (GAN) to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation; 
 obtain, by the QA system, a plurality of candidate answers to a natural language question, wherein each candidate answer comprises one or more ngrams; 
 generate, by the QA system, for each candidate answer in the plurality of candidate answers, a confidence score associated with the candidate answer based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN, wherein the confidence score represents a confidence that the candidate answer is a correct answer to the input natural language question; 
 select, by the QA system, at least one final answer to the input natural language question from the plurality of candidate answers based on the confidence scores associated with the candidate answers; and 
 output, by the QA system, the selected at least one final answer to the source computing device. 
 
     
     
       12. The computer program product of  claim 11 , wherein the generator neural network is trained to generate a BoN that represents ngrams present in a correct answer to the input question. 
     
     
       13. The computer program product of  claim 11 , wherein generating, for each candidate answer in the plurality of candidate answers, the confidence score associated with the candidate answer comprises scoring the candidate answer based on a degree of matching of the one or more ngrams in the candidate answers to ngrams in the BoN. 
     
     
       14. The computer program product of  claim 13 , wherein a candidate answer having a highest degree of matching with the BoN is selected as the at least one final answer for the input natural language question. 
     
     
       15. The computer program product of  claim 11 , wherein training the generator neural network of a GAN to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation comprises:
 configuring the generator neural network of the GAN to generate a bag-of-ngrams (BoN) output based on a noise vector input; 
 configuring a discriminator neural network of the GAN to receive a BoN input, where the BoN input is either the BoN output from the generator neural network or a BoN input associated with an actual portion of natural language text; 
 configuring the discriminator neural network of the GAN to output an indication of a probability as to whether the input BoN is from the actual portion of natural language text or is the BoN output of the generator neural network; and 
 training the generator neural network and discriminator neural network based on a feedback mechanism that compares the output indication from the discriminator neural network to an indicator of whether the input BoN is from the actual portion of natural language text of the BoN output of the generator neural network. 
 
     
     
       16. The computer program product of  claim 15 , wherein:
 the generator neural network produces the BoN output as a vector output, and wherein each vector slot in the vector output of the BoN output is set to a value indicative of a probability of whether a corresponding ngram is in the BoN, and 
 the discriminator neural network performs one or more statistical value analysis operations or feature extraction analysis operations on the BoN output of the generator neural network to score the BoN output and generate the indication of the probability as to whether the BoN output is from an actual portion of natural language text. 
 
     
     
       17. The computer program product of  claim 15 , wherein the generator neural network, during training of the generator neural network and discriminator neural network:
 receives a noise vector input; 
 projects and replicates the noise vector input to form a first matrix data structure; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a second matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the second matrix; 
 concatenates the first matrix and the second matrix to generate a concatenation matrix; 
 inputs each row of the concatenation matrix into a neural network; and 
 processes each row of the concatenation matrix through the neural network to generate the BoN output. 
 
     
     
       18. The computer program product of  claim 15 , wherein each row of the concatenation matrix comprises a first portion corresponding to the first matrix, and a second portion corresponding to the second matrix. 
     
     
       19. The computer program product of  claim 15 , wherein the discriminator neural network, during training of the generator neural network and discriminator neural network:
 receives the BoN input; 
 retrieves an embedding of each ngram in a vocabulary comprising a full set of ngrams that may be represented in the BoN; 
 generates a first matrix based on the retrieved embeddings, wherein each embedding is represented as a row in the first matrix; 
 multiplies the BoN input with the first matrix; 
 projects results of the multiplication of the BoN input with the first matrix to generate a second matrix; 
 performs sum pooling on the second matrix to generate a feature vector output; and 
 processes the feature vector output via a neural network to generate an output indicating whether or not the BoN input is from the actual portion of natural language text or is the BoN output of the generator neural network. 
 
     
     
       20. An apparatus comprising:
 at least one processor; and 
 at least one memory coupled to the at least one processor, wherein the at least one memory comprises instructions which, when executed by the at least one processor, configures the at least one processor to implement a Question Answering (QA) system, and causes the at least one processor to: 
 train a generator neural network of a generative adversarial network (GAN) to generate a bag-of-ngrams (BoN) output representing unlabeled data for performing a natural language processing operation; 
 obtain, by the QA system, a plurality of candidate answers to a natural language question, wherein each candidate answer comprises one or more ngrams; generate, by the QA system, for each candidate answer in the plurality of candidate answers, a confidence score associated with the candidate answer based on a comparison of the one or more ngrams in the candidate answer to ngrams in the BoN output of the generator neural network of the GAN, wherein the confidence score represents a confidence that the candidate answer is a correct answer to the input natural language question; 
 select, by the QA system, at least one final answer to the input natural language question from the plurality of candidate answers based on the confidence scores associated with the candidate answers; and 
 output, by the QA system, the selected at least one final answer to the source computing device.",International Business Machines Corporation,,,,20221025,20180712,20180712,US,US-201816033313-A,69138372,,
