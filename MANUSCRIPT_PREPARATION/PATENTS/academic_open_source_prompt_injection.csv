ID,Title,Type,Year,Authors/Organization,Source URL,Key Contribution,Mapped Gap(s)
1,Formalizing and Benchmarking Prompt Injection Attacks and Defenses,Paper,2024,Yupei Liu et al.,https://www.usenix.org/conference/usenixsecurity24/presentation/liu-yupei,Framework + benchmark of 5 attacks & 10 defenses; public platform,"Benchmark/Dataset, All"
2,Open-Prompt-Injection,Repo,2025,liu00222 (Yupei Liu),https://github.com/liu00222/Open-Prompt-Injection,Toolkit for implementing/evaluating prompt injection attacks & defenses,Benchmark/Tools
3,Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection,Paper,2023,Kai Greshake et al.,https://arxiv.org/abs/2302.12173,Introduces indirect prompt injection against LLM-integrated apps,Indirect/Agents
4,InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents,Paper,2024,Qiusi Zhan et al.,https://aclanthology.org/2024.findings-acl.624.pdf,Benchmark for indirect prompt injections across 30 LLM agents,"Indirect/Agents, Benchmark"
5,InjecAgent (code),Repo,2024,UIUC Kang Lab,https://github.com/uiuc-kang-lab/InjecAgent,Reference code/data for InjecAgent benchmark,Benchmark/Tools
6,Benchmarking and Defending Against Indirect Prompt Injection Attacks on LLM-Integrated Applications (BIPIA),Paper,2023,X. authors,https://arxiv.org/abs/2312.14197,First benchmark for indirect prompt injection in LLM-integrated apps,"Indirect/Agents, Benchmark"
7,StruQ: Defending Against Prompt Injection with Structured Queries,Paper,2024,Sizhe Chen et al.,https://arxiv.org/abs/2402.06363,Separates prompts vs data channels via structured queries,"Defense Technique, Multi-input"
8,Defending Against Indirect Prompt Injection Attacks With Spotlighting,Paper,2024,Keegan Hines et al.,https://arxiv.org/abs/2403.14720,Provenance-preserving transformations reduce IPI success <2% in tests,"Indirect/Agents, Defense Technique"
9,FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks,Paper,2024,Jiongxiao Wang et al.,https://arxiv.org/abs/2410.21492,Hash-based authentication tags for test-time defense; SOTA results,"Indirect/Agents, Test-time Defense"
10,FATH (code),Repo,2024,Jayfeather1024,https://github.com/Jayfeather1024/FATH,Official code for FATH test-time defense,Defense/Tools
11,AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents,Paper,2024,"Debenedetti, Zhang, Balunović, Beurer-Kellner, Fischer, Tramèr",https://arxiv.org/abs/2406.13352,Extensible evaluation environment for agent PI attacks/defenses,"Indirect/Agents, Benchmark"
12,AgentDojo (project site),Site/Repo,2024,ETH Zürich SPY Lab,https://agentdojo.spylab.ai/,Docs/results/quickstart for AgentDojo,Benchmark/Tools
13,OWASP LLM01:2025 Prompt Injection,Standard,2025,OWASP GenAI Security Project,https://genai.owasp.org/llmrisk/llm01-prompt-injection/,Risk definition & mitigations for LLM01,Industry/Standards
14,OWASP Cheat Sheet: LLM Prompt Injection Prevention,Guidance,2024,OWASP,https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html,Best practices & design controls to prevent PI,Standards/Guidance
15,NVIDIA NeMo Guardrails (GitHub),Repo,2024,NVIDIA,https://github.com/NVIDIA-NeMo/Guardrails,"Framework to protect LLM apps (PI, jailbreaks, PII, RAG policies)",Guardrails/Framework
16,NVIDIA NeMo Guardrails (Docs),Docs,2025,NVIDIA,https://docs.nvidia.com/nemo/guardrails/latest/getting-started.html,How to add guardrails incl. input validation & safety,Guardrails/Framework
17,Meta Llama Guard 2 (Model Card),Repo/Model,2024,Meta,https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md,Input/Output safeguard model for classification,Guardrails/Classifier
18,Meta-Llama-Guard-2-8B (Hugging Face),Model,2024,Meta,https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B,Deployed model weights for prompt/response classification,Guardrails/Classifier
19,PromptBench (GitHub),Repo,2024,Microsoft,https://github.com/microsoft/promptbench,Unified evaluation incl. adversarial prompts,Benchmark/Tools
20,PromptBench docs,Docs,2024,Microsoft,https://promptbench.readthedocs.io/en/latest/start/intro.html,Documentation on adversarial prompt attacks & evaluation,Benchmark/Tools
21,Guardrails (Python framework),Repo,2025,guardrails-ai,https://github.com/guardrails-ai/guardrails,Input/Output guards incl. PI detection; benchmarks (Index),Guardrails/Framework
22,llm-security-prompt-injection,Repo,2023,sinanw,https://github.com/sinanw/llm-security-prompt-injection,Binary classification of malicious prompts,Dataset/Classifier
23,Protect AI: DeBERTa-v3 Prompt Injection (HF model),Model,2024,Protect AI,https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2,Fine-tuned classifier for PI detection,Guardrails/Classifier
24,LLM Guard (Protect AI) prompt-injection scanner,Docs,2024,Protect AI,https://protectai.github.io/llm-guard/input_scanners/prompt_injection/,How to use PI scanner in LLM Guard,Guardrails/Framework
25,PreambleAI/prompt-injection-defense (HF model),Model,2024,Preamble AI,https://huggingface.co/PreambleAI/prompt-injection-defense,Lightweight classifier for untrusted input detection,Guardrails/Classifier
26,Wired: Generative AI’s Biggest Security Flaw Is Not Easy to Fix,News/Industry,2023,WIRED,https://www.wired.com/story/generative-ai-prompt-injection-hacking,Context on indirect PI risks & real-world incidents,Industry Awareness
27,Microsoft: How Microsoft defends against indirect prompt injection attacks,Industry Blog,2025,Microsoft MSRC,https://www.microsoft.com/en-us/msrc/blog/2025/07/how-microsoft-defends-against-indirect-prompt-injection-attacks,Defense-in-depth approach (probabilistic + deterministic),Industry Practices
28,HiddenLayer: Policy Puppetry — a universal prompt injection bypass,Industry Research,2025,HiddenLayer,https://hiddenlayer.com/innovation-hub/novel-universal-bypass-for-all-major-llms/,Demonstrates universal PI bypass across major models,Attack Techniques
29,safe-guard-prompt-injection (HF dataset),Dataset,2024,xTRam1,https://huggingface.co/datasets/xTRam1/safe-guard-prompt-injection,10k samples across PI categories for training/testing,Dataset
30,Prompt Injection Benign/Injection Dataset (HF),Dataset,2024,darkknight25,https://huggingface.co/datasets/darkknight25/Prompt_Injection_Benign_Prompt_Dataset,Dataset of injection vs benign prompts,Dataset
31,"OWASP LLM01 Prompt Injection (source, GitHub)",Standard/Source,2025,OWASP,https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/blob/main/2_0_vulns/LLM01_PromptInjection.md,Source of the LLM01 risk page,Standards/Guidance
32,NeMo Guardrails | NVIDIA Developer,Docs,2025,NVIDIA,https://developer.nvidia.com/nemo-guardrails,Overview of LLM vulnerability protection incl. PI,Guardrails/Framework
33,IBM Granite Guardian (GitHub),Repo/Model,2024,IBM,https://github.com/ibm-granite/granite-guardian,Judging prompts/responses incl. jailbreak & RAG risks,Guardrails/Classifier
34,Granite Guardian (HF model),Model,2025,IBM,https://huggingface.co/ibm-granite/granite-guardian-3.3-8b,Model card & weights for Granite Guardian 3.3 8B,Guardrails/Classifier
35,Prompt-Injection PoC: LLM Security Gateway,Repo,2025,Tobias Rehermann,https://github.com/TobiasRehermann/Prompt-Injection-PoC-LLM-Security-Gateway,Simple pre-LLM gateway filter showcasing PI detection,Gateway/MVP
36,LangChain Guardrails docs,Docs,2025,LangChain,https://docs.langchain.com/oss/python/langchain/guardrails,How to add guardrails including PI checks within LangChain apps,Guardrails/Framework
37,Protect AI: Prompt Injection Solutions Benchmark (HF Space),Benchmark,2025,Protect AI,https://huggingface.co/spaces/protectai/prompt-injection-benchmark,Public benchmark interface comparing PI detectors,Benchmark/Tools
