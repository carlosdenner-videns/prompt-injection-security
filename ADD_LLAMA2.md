# Adding LLaMA-2 to Your Experiments

## Why LLaMA-2 Requires Special Access

LLaMA-2 is a **gated model** from Meta. You need explicit permission to use it.

## Steps to Get Access

### 1. Request Access from Meta

Visit: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf

Click: **"Request Access"** button

Fill out the form:
- Your name
- Organization (can be "Independent Researcher" or your university)
- Use case (e.g., "Academic research on prompt injection vulnerabilities")
- Agree to terms

### 2. Wait for Approval

- **Timeline**: Usually 1-48 hours (sometimes instant, sometimes takes a day)
- **Notification**: You'll receive email when approved
- **Check status**: Visit model page, look for "You have access" message

### 3. Verify Access

Once approved:

```powershell
# Test if you can access the model
huggingface-cli whoami

# Try downloading just the config (quick test)
python -c "from transformers import AutoConfig; AutoConfig.from_pretrained('meta-llama/Llama-2-7b-chat-hf')"
```

If no error â†’ Access granted! âœ…

## Adding LLaMA-2 to Experiments

### Part A (RAG-Borne Injection)

Edit `partA_experiment.py` line 416:

**Current (Falcon only):**
```python
models=["falcon-7b"],  # Use Falcon only (no access restrictions)
```

**After LLaMA-2 access:**
```python
models=["falcon-7b", "llama2-7b"],  # Both models
```

### Part B (Schema Smuggling)

Edit `partB_experiment.py` line 512:

**Current (Falcon only):**
```python
models=["falcon-7b"],  # Use Falcon only (no access restrictions)
```

**After LLaMA-2 access:**
```python
models=["falcon-7b", "llama2-7b"],  # Both models
```

## Re-running Experiments with Both Models

### Option 1: Run Both Models Together

After adding LLaMA-2 to the model lists:

```powershell
# Part A - will run Falcon then LLaMA-2
python partA_experiment.py

# Part B - will run Falcon then LLaMA-2  
python partB_experiment.py
```

**Time**: ~2x longer (both models)

### Option 2: Run LLaMA-2 Only (Keep Falcon Results)

If you already have Falcon results and want to add LLaMA-2:

**Create a separate script:**

`partA_experiment_llama2.py`:
```python
if __name__ == "__main__":
    experiment = RAGInjectionExperiment(
        kb_path="partA_kb.jsonl",
        models=["llama2-7b"],  # LLaMA-2 only
        seed=1337
    )
    
    experiment.run_experiment(
        num_queries=200,
        max_new_tokens=150,
        output_file="partA_results_llama2.json"
    )
```

Run:
```powershell
python partA_experiment_llama2.py
```

**Then merge results:**
```python
import json

# Load both result files
with open("partA_results.json") as f:
    falcon_results = json.load(f)

with open("partA_results_llama2.json") as f:
    llama_results = json.load(f)

# Combine
all_results = falcon_results + llama_results

# Save merged
with open("partA_results_combined.json", "w") as f:
    json.dump(all_results, f, indent=2)
```

## Alternative: Use Different LLaMA Variant

If you can't get LLaMA-2 access, consider:

### 1. LLaMA-3 (Also Gated)
- `meta-llama/Meta-Llama-3-8B-Instruct`
- Similar gating process
- Newer, potentially better performance

### 2. Open Alternatives (No Gating)
- **Mistral-7B**: `mistralai/Mistral-7B-Instruct-v0.2`
- **Gemma-7B**: `google/gemma-7b-it`  
- **Phi-3**: `microsoft/Phi-3-mini-4k-instruct`

**To use Mistral instead:**
```python
models=["falcon-7b", "mistral-7b"]  # In model_utils.py, add:
# "mistral-7b": "mistralai/Mistral-7B-Instruct-v0.2"
```

## Current Status (Falcon Only)

**âœ… What works now:**
- Part A: 200 queries on Falcon-7b (~1-1.5 hours)
- Part B: 72 cases on Falcon-7b (~30-45 minutes)
- All analysis and visualization
- Complete baseline results

**ðŸ“Š Publication Note:**
- One model is sufficient for proof-of-concept
- Can note "LLaMA-2 pending access" in paper
- Add LLaMA-2 results in revision if needed
- Multi-model comparison strengthens paper but not required

## When You Get Access

**Quick checklist:**
1. âœ… Verify access with test command above
2. âœ… Update model lists in both experiment files
3. âœ… Re-run experiments (or run LLaMA-2 separately)
4. âœ… Run analysis again to include both models
5. âœ… Update visualizations with comparison

**Expected differences:**
- LLaMA-2 typically: Slightly lower ASR (better at following instructions)
- Falcon typically: Slightly higher ASR (more vulnerable)
- Both: Should show >50% ASR for most attack types

---

**For now:** Proceed with Falcon-7b! You'll get valid baseline results. ðŸš€
